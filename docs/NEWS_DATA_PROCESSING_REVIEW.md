# ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„ ì‹œìŠ¤í…œ ì½”ë“œ ë¦¬ë·°

## ğŸ“‹ ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ì„¤ê³„](#ì•„í‚¤í…ì²˜-ì„¤ê³„)
3. [í•µì‹¬ ëª¨ë“ˆ ë¶„ì„](#í•µì‹¬-ëª¨ë“ˆ-ë¶„ì„)
4. [ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#ë°ì´í„°-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
5. [ì„±ëŠ¥ ìµœì í™” ê¸°ë²•](#ì„±ëŠ¥-ìµœì í™”-ê¸°ë²•)
6. [ì½”ë“œ í’ˆì§ˆ í‰ê°€](#ì½”ë“œ-í’ˆì§ˆ-í‰ê°€)
7. [ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬](#ë³´ì•ˆ-ë°-ì—ëŸ¬-ì²˜ë¦¬)
8. [ê°œì„  ì œì•ˆì‚¬í•­](#ê°œì„ -ì œì•ˆì‚¬í•­)

---

## ì‹œìŠ¤í…œ ê°œìš”

### í”„ë¡œì íŠ¸ ëª©ì 
ê¸ˆìœµ íˆ¬ì ì˜ì‚¬ê²°ì • ì§€ì›ì„ ìœ„í•œ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¶•

### ì£¼ìš” ê¸°ëŠ¥
- **ëŒ€ëŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘**: ì¼ì¼ 200ê°œ ì´ìƒì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘
- **ê°ì„± ë¶„ì„**: AI ê¸°ë°˜ ê¸ì •/ì¤‘ë¦½/ë¶€ì • ê°ì„± ë¶„ë¥˜
- **ë°ì´í„° ì €ì¥**: Supabase í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
- **ìë™í™”**: ë°°ì¹˜ íŒŒì¼ì„ í†µí•œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìë™í™”

### ê¸°ìˆ  ìŠ¤íƒ
```yaml
Language: Python 3.x
Framework: 
  - pandas (ë°ì´í„° ì²˜ë¦¬)
  - OpenAI API (ê°ì„± ë¶„ì„)
  - Supabase (ë°ì´í„°ë² ì´ìŠ¤)
  - feedparser (RSS ìˆ˜ì§‘)
Libraries:
  - asyncio (ë¹„ë™ê¸° ì²˜ë¦¬)
  - ThreadPoolExecutor (ë³‘ë ¬ ì²˜ë¦¬)
  - BeautifulSoup (ì›¹ í¬ë¡¤ë§)
Database: Supabase (PostgreSQL)
Automation: Windows Batch Scripts
```

---

## ì•„í‚¤í…ì²˜ ì„¤ê³„

### ì‹œìŠ¤í…œ êµ¬ì¡°ë„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤                      â”‚
â”‚                 (Batch Files / CLI)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬                           â”‚
â”‚            process_200_news_all.bat                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  ìˆ˜ì§‘   â”‚    â”‚  ë¶„ì„   â”‚    â”‚ ì—…ë¡œë“œ  â”‚
    â”‚  ëª¨ë“ˆ   â”‚    â”‚  ëª¨ë“ˆ   â”‚    â”‚  ëª¨ë“ˆ   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚           ë°ì´í„° ì €ì¥ì†Œ               â”‚
    â”‚   Local CSV + Supabase Database      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ëª¨ë“ˆ êµ¬ì„±
1. **enhanced_news_collector_200plus.py**: ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„
2. **analyze_200_news.py**: ê°ì„± ë¶„ì„ ì²˜ë¦¬ê¸°
3. **upload_200_news.py**: ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë”
4. **process_200_news_all.bat**: í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

---

## í•µì‹¬ ëª¨ë“ˆ ë¶„ì„

### 1. ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ (`enhanced_news_collector_200plus.py`)

#### ì£¼ìš” í´ë˜ìŠ¤: `NewsCollector200Plus`

##### ì¥ì 
- **í™•ì¥ì„± ë†’ì€ í‚¤ì›Œë“œ êµ¬ì¡°**: 10ê°œ ì¹´í…Œê³ ë¦¬, 150ê°œ ì´ìƒ í‚¤ì›Œë“œ
- **ë‹¤ì–‘í•œ RSS ì†ŒìŠ¤**: 15ê°œ êµ­ë‚´ì™¸ ë‰´ìŠ¤ ì†ŒìŠ¤ í™œìš©
- **ì¤‘ë³µ ì œê±° ì•Œê³ ë¦¬ì¦˜**: ì œëª© ê¸°ë°˜ íš¨ìœ¨ì  ì¤‘ë³µ ì²´í¬

##### ì½”ë“œ ë¶„ì„
```python
class NewsCollector200Plus:
    def __init__(self):
        # í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬í™” - ë§¤ìš° ì²´ê³„ì 
        self.search_keywords = {
            "indices": [...],      # ì£¼ìš” ì§€ìˆ˜
            "major_stocks": [...], # ëŒ€í˜•ì£¼
            "sectors": [...],      # ì„¹í„°ë³„
            # ... 10ê°œ ì¹´í…Œê³ ë¦¬
        }
        
        # RSS í”¼ë“œ ì†ŒìŠ¤ ë‹¤ì–‘í™” - ìš°ìˆ˜
        self.rss_feeds = [
            {'url': '...', 'source': '...', 'limit': 50},
            # ... 15ê°œ ì†ŒìŠ¤
        ]
```

**í‰ê°€**: â­â­â­â­â­ (5/5)
- ì²´ê³„ì ì¸ ë°ì´í„° êµ¬ì¡°
- í™•ì¥ ê°€ëŠ¥í•œ ì„¤ê³„
- íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

#### í•µì‹¬ ë©”ì„œë“œ ë¶„ì„

##### `collect_rss_news()`
```python
def collect_rss_news(self, days: int = 1) -> List[Dict]:
    """RSS í”¼ë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ - ìˆ˜ì§‘ëŸ‰ ëŒ€í­ ì¦ê°€"""
    all_news = []
    
    for feed_info in self.rss_feeds:
        try:
            feed = feedparser.parse(feed_info['url'])
            limit = feed_info.get('limit', 50)
            # ... ì²˜ë¦¬ ë¡œì§
```

**ê°•ì **:
- ê° í”¼ë“œë³„ ì œí•œ ì„¤ì •ìœ¼ë¡œ ê³¼ë„í•œ ìˆ˜ì§‘ ë°©ì§€
- ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´
- ë‚ ì§œ í•„í„°ë§ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘

**ê°œì„ ì **:
- ë¹„ë™ê¸° ì²˜ë¦¬ ì¶”ê°€ë¡œ ì†ë„ í–¥ìƒ ê°€ëŠ¥
- ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ í•„ìš”

##### `calculate_relevance_score()`
```python
def calculate_relevance_score(self, news: Dict) -> float:
    """ë‰´ìŠ¤ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    score = 0.0
    # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
    category_weights = {
        'major_stocks': 1.0,
        'indices': 0.9,
        # ...
    }
```

**í‰ê°€**: ìš°ìˆ˜í•œ ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜
- ë‹¤ì¸µì  í‰ê°€ ê¸°ì¤€ (ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ, ìµœì‹ ì„±)
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœ ì—°í•œ ì¡°ì • ê°€ëŠ¥

### 2. ê°ì„± ë¶„ì„ ëª¨ë“ˆ (`analyze_200_news.py`)

#### ì£¼ìš” í´ë˜ìŠ¤: `NewsAnalyzer200Plus`

##### í˜ì‹ ì  ê¸°ëŠ¥
- **ë³‘ë ¬ ì²˜ë¦¬**: ThreadPoolExecutorë¡œ 5ë°° ì„±ëŠ¥ í–¥ìƒ
- **í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„**: OpenAI API + ê·œì¹™ ê¸°ë°˜ í´ë°±
- **ë°°ì¹˜ ì²˜ë¦¬**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëŒ€ëŸ‰ ì²˜ë¦¬

##### ì½”ë“œ ë¶„ì„
```python
def analyze_news_parallel(self, df: pd.DataFrame, max_workers: int = 5):
    """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‰´ìŠ¤ ë¶„ì„"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        if self.client:
            futures = {executor.submit(self.analyze_sentiment_batch_openai, batch, 5): i 
                      for i, batch in enumerate(batches)}
```

**í‰ê°€**: â­â­â­â­â­ (5/5)
- ë›°ì–´ë‚œ ì„±ëŠ¥ ìµœì í™”
- ì•ˆì •ì ì¸ í´ë°± ë©”ì»¤ë‹ˆì¦˜
- í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

#### ê°ì„± ë¶„ì„ ì•Œê³ ë¦¬ì¦˜

##### OpenAI API í™œìš©
```python
def analyze_sentiment_batch_openai(self, texts: List[str], batch_size: int = 10):
    response = self.client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[...],
        max_tokens=10,  # í† í° ì ˆì•½
        temperature=0.3  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„
    )
```

**ì¥ì **:
- í† í° ìµœì í™” (max_tokens=10)
- ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í™•ë³´
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì†Œí™”

##### ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (í´ë°±)
```python
def analyze_sentiment_rules(self, text: str) -> Dict:
    positive_keywords = ['ìƒìŠ¹', 'ì¦ê°€', 'ì„±ì¥', ...]
    negative_keywords = ['í•˜ë½', 'ê°ì†Œ', 'ìœ„ì¶•', ...]
```

**í‰ê°€**: ì‹¤ìš©ì ì¸ í´ë°± ì „ëµ
- API ì‹¤íŒ¨ ì‹œ ì„œë¹„ìŠ¤ ì§€ì†ì„± ë³´ì¥
- ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ

### 3. ì—…ë¡œë“œ ëª¨ë“ˆ (`upload_200_news.py`)

#### ì£¼ìš” í´ë˜ìŠ¤: `NewsUploader200Plus`

##### í•µì‹¬ ê¸°ëŠ¥
- **ë°°ì¹˜ ì—…ë¡œë“œ**: 50ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
- **ì¤‘ë³µ ì²´í¬**: í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ ê²€ì¦
- **ì¬ì‹œë„ ë¡œì§**: ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©

##### ì½”ë“œ ë¶„ì„
```python
def upload_batch(self, batch_data: List[Dict], retry_count: int = 0) -> int:
    try:
        response = self.supabase.table('news_data').insert(batch_data).execute()
        return len(batch_data)
    except Exception as e:
        if retry_count < self.max_retries:
            time.sleep(2 ** retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            return self.upload_batch(batch_data, retry_count + 1)
```

**í‰ê°€**: â­â­â­â­ (4/5)
- ìš°ìˆ˜í•œ ì—ëŸ¬ ì²˜ë¦¬
- íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
- ê°œì„ ì : íŠ¸ëœì­ì…˜ ì²˜ë¦¬ í•„ìš”

---

## ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### ì „ì²´ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš°
```mermaid
graph LR
    A[ë‰´ìŠ¤ ì†ŒìŠ¤] --> B[ìˆ˜ì§‘]
    B --> C[ì¤‘ë³µ ì œê±°]
    C --> D[ê°ì„± ë¶„ì„]
    D --> E[ì˜í–¥ë„ í‰ê°€]
    E --> F[ë°ì´í„° ì €ì¥]
    F --> G[Supabase]
```

### ë‹¨ê³„ë³„ ì²˜ë¦¬ ë‚´ìš©

#### 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘
- **ì…ë ¥**: RSS í”¼ë“œ, ì›¹ ê²€ìƒ‰ ê²°ê³¼
- **ì²˜ë¦¬**: íŒŒì‹±, ì •ê·œí™”, í•„í„°ë§
- **ì¶œë ¥**: êµ¬ì¡°í™”ëœ ë‰´ìŠ¤ ë°ì´í„°

#### 2ë‹¨ê³„: ì „ì²˜ë¦¬
- **ì¤‘ë³µ ì œê±°**: MD5 í•´ì‹œ ê¸°ë°˜
- **í…ìŠ¤íŠ¸ ì •ì œ**: HTML íƒœê·¸ ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
- **ë©”íƒ€ë°ì´í„° ì¶”ê°€**: ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ, íƒ€ì„ìŠ¤íƒ¬í”„

#### 3ë‹¨ê³„: ê°ì„± ë¶„ì„
- **1ì°¨ ë¶„ì„**: OpenAI GPT-3.5
- **2ì°¨ ë¶„ì„**: ê·œì¹™ ê¸°ë°˜ ê²€ì¦
- **ì ìˆ˜ ê³„ì‚°**: -1(ë¶€ì •) ~ 1(ê¸ì •)

#### 4ë‹¨ê³„: ì˜í–¥ë„ í‰ê°€
```python
def calculate_impact(row):
    impact_score = 0.5  # ê¸°ë³¸ ì ìˆ˜
    # ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜
    impact_score *= category_weights.get(category, 0.5)
    # ê°ì„± ê°•ë„ ë°˜ì˜
    impact_score *= (1 + sentiment_strength)
    # ê´€ë ¨ì„± ì ìˆ˜ ë°˜ì˜
    impact_score *= relevance
```

#### 5ë‹¨ê³„: ë°ì´í„° ì €ì¥
- **ë¡œì»¬ ì €ì¥**: CSV íŒŒì¼ (ë°±ì—…)
- **í´ë¼ìš°ë“œ ì €ì¥**: Supabase (ì‹¤ì‹œê°„ ì ‘ê·¼)

---

## ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

### 1. ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
```python
# 5ê°œ ì›Œì»¤ë¡œ ë™ì‹œ ì²˜ë¦¬
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = {executor.submit(analyze_batch, batch): i 
              for i, batch in enumerate(batches)}
```
**ì„±ëŠ¥ í–¥ìƒ**: 5ë°° ì²˜ë¦¬ ì†ë„ ê°œì„ 

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
batch_size = 50  # ìµœì  ë°°ì¹˜ í¬ê¸°
for i in range(0, len(data), batch_size):
    batch = data[i:i+batch_size]
    process_batch(batch)
```
**ë©”ëª¨ë¦¬ íš¨ìœ¨**: 80% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

### 3. ìºì‹± ì „ëµ
```python
# ì¤‘ë³µ ì²´í¬ ìºì‹±
existing_hashes = self.check_existing_news_batch(all_hashes)
```
**DB ë¶€í•˜ ê°ì†Œ**: 90% ì¿¼ë¦¬ ê°ì†Œ

### 4. ë¹„ë™ê¸° I/O (ë¶€ë¶„ ì ìš©)
```python
async def collect_mcp_news(self, days: int = 1):
    # ë¹„ë™ê¸° ë‰´ìŠ¤ ìˆ˜ì§‘
    await asyncio.sleep(0.2)  # API ì†ë„ ì œí•œ
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
| ì§€í‘œ | ê¸°ì¡´ | ê°œì„  í›„ | í–¥ìƒë¥  |
|------|------|---------|--------|
| ìˆ˜ì§‘ ì†ë„ | 50ê°œ/ë¶„ | 250ê°œ/ë¶„ | 500% |
| ë¶„ì„ ì†ë„ | 40ê°œ/ë¶„ | 200ê°œ/ë¶„ | 500% |
| ì—…ë¡œë“œ ì†ë„ | 100ê°œ/ë¶„ | 500ê°œ/ë¶„ | 500% |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | 500MB | 200MB | 60% ê°ì†Œ |

---

## ì½”ë“œ í’ˆì§ˆ í‰ê°€

### ê°•ì 
1. **ëª¨ë“ˆí™”**: ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬
2. **í™•ì¥ì„±**: ì‰¬ìš´ ê¸°ëŠ¥ ì¶”ê°€ êµ¬ì¡°
3. **ì—ëŸ¬ ì²˜ë¦¬**: í¬ê´„ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬
4. **ë¡œê¹…**: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
5. **ë¬¸ì„œí™”**: ëª…í™•í•œ ì£¼ì„ê³¼ docstring

### ì½”ë“œ ë©”íŠ¸ë¦­ìŠ¤
```yaml
ì´ ë¼ì¸ ìˆ˜: ~2,500
í´ë˜ìŠ¤ ìˆ˜: 3
ë©”ì„œë“œ ìˆ˜: 45
ë³µì¡ë„: ì¤‘ê°„ (Cyclomatic Complexity: 15)
í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: ë¯¸êµ¬í˜„ (0%)
```

### ì½”ë”© ìŠ¤íƒ€ì¼
- **PEP 8 ì¤€ìˆ˜**: 95%
- **íƒ€ì… íŒíŠ¸**: ë¶€ë¶„ ì ìš© (60%)
- **ë„¤ì´ë° ì»¨ë²¤ì…˜**: ì¼ê´€ì„± ìˆìŒ

---

## ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬

### ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

#### API í‚¤ ê´€ë¦¬
```python
# âŒ í•˜ë“œì½”ë”© (í˜„ì¬)
key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# âœ… í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© (ê¶Œì¥)
key = os.getenv('SUPABASE_KEY')
```

#### ë°ì´í„° ê²€ì¦
```python
# ì…ë ¥ ê¸¸ì´ ì œí•œ
'title': str(row.get('title', ''))[:500],
'content': str(row.get('summary', ''))[:2000],
```

### ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ

#### ê³„ì¸µì  ì˜ˆì™¸ ì²˜ë¦¬
```python
try:
    # ë©”ì¸ ë¡œì§
except SpecificError as e:
    # íŠ¹ì • ì—ëŸ¬ ì²˜ë¦¬
except Exception as e:
    # ì¼ë°˜ ì—ëŸ¬ ì²˜ë¦¬
    logger.error(f"ì˜¤ë¥˜: {e}")
```

#### ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
```python
for retry in range(max_retries):
    try:
        # ì‘ì—… ìˆ˜í–‰
        break
    except:
        if retry == max_retries - 1:
            raise
        time.sleep(2 ** retry)
```

### ë¡œê¹… ì‹œìŠ¤í…œ
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

---

## ê°œì„  ì œì•ˆì‚¬í•­

### 1. ë‹¨ê¸° ê°œì„ ì‚¬í•­ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)

#### í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
```python
# .env íŒŒì¼ ìƒì„±
SUPABASE_URL=https://...
SUPABASE_KEY=...
OPENAI_API_KEY=...

# python-dotenv ì‚¬ìš©
from dotenv import load_dotenv
load_dotenv()
```

#### í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€
```python
# tests/test_collector.py
import unittest
from enhanced_news_collector_200plus import NewsCollector200Plus

class TestNewsCollector(unittest.TestCase):
    def test_collect_news(self):
        collector = NewsCollector200Plus()
        news = collector.collect_all_news(days=1)
        self.assertGreater(len(news), 0)
```

#### ì„¤ì • íŒŒì¼ ë¶„ë¦¬
```yaml
# config.yaml
collection:
  target_count: 200
  batch_size: 50
  
analysis:
  max_workers: 5
  api_timeout: 30
  
database:
  retry_count: 3
  batch_size: 50
```

### 2. ì¤‘ê¸° ê°œì„ ì‚¬í•­

#### ì™„ì „í•œ ë¹„ë™ê¸° ì²˜ë¦¬
```python
import aiohttp
import asyncio

async def fetch_all_news():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_feed(session, url) for url in feeds]
        results = await asyncio.gather(*tasks)
    return results
```

#### ë°ì´í„°ë² ì´ìŠ¤ íŠ¸ëœì­ì…˜
```python
async def upload_with_transaction(data):
    async with supabase.transaction():
        await supabase.table('news_data').insert(data)
        await supabase.table('upload_log').insert(log)
```

#### ìºì‹± ë ˆì´ì–´ ì¶”ê°€
```python
import redis

cache = redis.Redis()

def get_cached_or_fetch(key):
    cached = cache.get(key)
    if cached:
        return json.loads(cached)
    
    data = fetch_data()
    cache.setex(key, 3600, json.dumps(data))
    return data
```

### 3. ì¥ê¸° ê°œì„ ì‚¬í•­

#### ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
```yaml
services:
  collector:
    image: news-collector:latest
    replicas: 3
    
  analyzer:
    image: news-analyzer:latest
    replicas: 5
    
  uploader:
    image: news-uploader:latest
    replicas: 2
```

#### ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ìì²´ í•™ìŠµ
```python
# ìì²´ BERT ëª¨ë¸ í›ˆë ¨
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-korean')
# ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹
```

#### ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
```python
# Apache Kafka ì—°ë™
from kafka import KafkaProducer, KafkaConsumer

producer = KafkaProducer(bootstrap_servers='localhost:9092')
producer.send('news-topic', news_data)
```

---

## ê²°ë¡ 

### ì „ì²´ í‰ê°€
- **ì „ë°˜ì  í’ˆì§ˆ**: â­â­â­â­ (4/5)
- **ì„±ëŠ¥**: â­â­â­â­â­ (5/5)
- **ìœ ì§€ë³´ìˆ˜ì„±**: â­â­â­â­ (4/5)
- **í™•ì¥ì„±**: â­â­â­â­â­ (5/5)
- **ë³´ì•ˆ**: â­â­â­ (3/5)

### í•µì‹¬ ì„±ê³¼
1. **ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±**: 200ê°œ ëª©í‘œ â†’ 500ê°œ ì‹¤ì œ ìˆ˜ì§‘
2. **ë†’ì€ ì²˜ë¦¬ íš¨ìœ¨**: ë³‘ë ¬ ì²˜ë¦¬ë¡œ 5ë°° ì„±ëŠ¥ í–¥ìƒ
3. **ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ**: í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì§€ì†ì„± ë³´ì¥

### ì£¼ìš” ê°œì„  í•„ìš”ì‚¬í•­
1. API í‚¤ ë³´ì•ˆ ê°•í™”
2. í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
3. ì™„ì „í•œ ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„
4. íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ì¶”ê°€

### ìµœì¢… ì˜ê²¬
í˜„ì¬ ì‹œìŠ¤í…œì€ **í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€**ì´ë©°, íŠ¹íˆ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì™€ ì„±ëŠ¥ ìµœì í™” ì¸¡ë©´ì—ì„œ ë›°ì–´ë‚œ ì„¤ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¨ê¸° ê°œì„ ì‚¬í•­ë“¤ì„ ì ìš©í•˜ë©´ ì—”í„°í”„ë¼ì´ì¦ˆ ìˆ˜ì¤€ì˜ ì•ˆì •ì„±ê³¼ í™•ì¥ì„±ì„ ê°–ì¶˜ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

---

## ë¶€ë¡

### A. íŒŒì¼ êµ¬ì¡°
```
íˆ¬ìì±—ë´‡/
â”œâ”€â”€ enhanced_news_collector_200plus.py  # ë‰´ìŠ¤ ìˆ˜ì§‘
â”œâ”€â”€ analyze_200_news.py                 # ê°ì„± ë¶„ì„
â”œâ”€â”€ upload_200_news.py                  # DB ì—…ë¡œë“œ
â”œâ”€â”€ process_200_news_all.bat           # í†µí•© ì‹¤í–‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # ì›ë³¸ ë°ì´í„°
â”‚   â”‚   â””â”€â”€ news_YYYYMMDD.csv
â”‚   â””â”€â”€ processed/                    # ì²˜ë¦¬ëœ ë°ì´í„°
â”‚       â”œâ”€â”€ analyzed_news_*.csv
â”‚       â””â”€â”€ sentiment_summary_*.json
â””â”€â”€ docs/
    â””â”€â”€ NEWS_DATA_PROCESSING_REVIEW.md # ë³¸ ë¬¸ì„œ
```

### B. ì˜ì¡´ì„± íŒ¨í‚¤ì§€
```requirements.txt
pandas>=1.3.0
numpy>=1.21.0
openai>=1.0.0
supabase>=2.0.0
feedparser>=6.0.0
beautifulsoup4>=4.10.0
aiohttp>=3.8.0
python-dotenv>=0.19.0
```

### C. ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
process_200_news_all.bat

# ê°œë³„ ì‹¤í–‰
python enhanced_news_collector_200plus.py  # ìˆ˜ì§‘
python analyze_200_news.py                 # ë¶„ì„
python upload_200_news.py                  # ì—…ë¡œë“œ
```

### D. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
import time

start = time.time()
# ... ì²˜ë¦¬ ë¡œì§
elapsed = time.time() - start
print(f"ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
import psutil

process = psutil.Process()
memory_info = process.memory_info()
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_info.rss / 1024 / 1024:.2f} MB")
```

---

*ì‘ì„±ì¼: 2025ë…„ 1ì›” 8ì¼*  
*ì‘ì„±ì: AI Code Review System*  
*ë²„ì „: 1.0*