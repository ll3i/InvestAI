# Pixie ë‰´ìŠ¤/ì´ìŠˆ (News & Issues)

## ê°œìš”
ë‰´ìŠ¤/ì´ìŠˆ í˜ì´ì§€ëŠ” ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘, AI ê¸°ë°˜ ê°ì„± ë¶„ì„, ê°œì¸í™”ëœ ë‰´ìŠ¤ íë ˆì´ì…˜ì„ ì œê³µí•˜ëŠ” ì¢…í•© ê¸ˆìœµ ì •ë³´ í”Œë«í¼ì…ë‹ˆë‹¤. BERT ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ì™€ TF-IDF ë²¡í„°í™”ë¥¼ í™œìš©í•˜ì—¬ íˆ¬ì ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

## í˜ì´ì§€ êµ¬ì¡°

### 1. ë©”ì¸ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ (`/news`)
```
í—¤ë”: "ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ & ì‹œì¥ ì´ìŠˆ"
ì„œë¸Œí—¤ë”: "AIê°€ ë¶„ì„í•œ ì˜¤ëŠ˜ì˜ íˆ¬ì ì¸ì‚¬ì´íŠ¸"
ì£¼ìš” ì„¹ì…˜:
- í—¤ë“œë¼ì¸ ë‰´ìŠ¤ (Top 5)
- ì‹¤ì‹œê°„ ë‰´ìŠ¤ í”¼ë“œ
- ì¢…ëª©ë³„ ë‰´ìŠ¤
- ê°ì„± ë¶„ì„ í˜„í™©
- í‚¤ì›Œë“œ íŠ¸ë Œë“œ
```

## ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ

### 1. ë°ì´í„° ì†ŒìŠ¤ ë° ìˆ˜ì§‘ ì—”ì§„
```python
class NewsCollector:
    def __init__(self):
        self.sources = {
            'naver_finance': 'https://finance.naver.com/news/news_list.nhn',
            'daum_finance': 'https://finance.daum.net/news',
            'yonhap': 'https://www.yna.co.kr/economy/all',
            'hankyung': 'https://www.hankyung.com/finance',
            'mk': 'https://www.mk.co.kr/news/stock/',
            'edaily': 'https://www.edaily.co.kr/news/stock',
            'reuters_kr': 'https://kr.reuters.com/finance'
        }
        self.rss_feeds = [
            'http://rss.hankyung.com/feed/finance.xml',
            'http://rss.mk.co.kr/rss/30100041.xml',
            'http://rss.edaily.co.kr/stock_news.xml'
        ]
        
    def collect_news(self):
        """ë©€í‹°ì†ŒìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        # RSS í”¼ë“œ ìˆ˜ì§‘
        for feed_url in self.rss_feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:20]:  # ìµœì‹  20ê°œ
                    news_item = {
                        'title': entry.title,
                        'summary': entry.get('summary', ''),
                        'link': entry.link,
                        'published': entry.get('published_parsed'),
                        'source': feed.feed.title,
                        'category': self.categorize_news(entry.title)
                    }
                    all_news.append(news_item)
            except Exception as e:
                logger.error(f"RSS ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ (ì¶”ê°€ ì†ŒìŠ¤)
        for source_name, url in self.sources.items():
            news = self.scrape_news_site(url, source_name)
            all_news.extend(news)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_news = self.remove_duplicates(all_news)
        sorted_news = sorted(unique_news, 
                           key=lambda x: x['published'], 
                           reverse=True)
        
        return sorted_news
```

### 2. ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë°
```python
class NewsStreamProcessor:
    def __init__(self):
        self.kafka_consumer = KafkaConsumer(
            'financial-news',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
    async def stream_news(self):
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë°"""
        async for message in self.kafka_consumer:
            news_data = message.value
            
            # ì‹¤ì‹œê°„ ì²˜ë¦¬
            processed = await self.process_news(news_data)
            
            # ì›¹ì†Œì¼“ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡
            await websocket.send(json.dumps({
                'type': 'news_update',
                'data': processed
            }))
    
    async def process_news(self, news):
        """ë‰´ìŠ¤ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        # ê°ì„± ë¶„ì„
        sentiment = await self.analyze_sentiment(news['content'])
        
        # ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
        related_stocks = self.extract_stock_mentions(news['content'])
        
        # ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ë§
        importance = self.calculate_importance(news)
        
        return {
            **news,
            'sentiment': sentiment,
            'related_stocks': related_stocks,
            'importance': importance,
            'processed_at': datetime.now().isoformat()
        }
```

## AI ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ

### 1. BERT ê¸°ë°˜ ê°ì„± ë¶„ì„
```python
class NewsSentimentAnalyzer:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('klue/bert-base')
        self.model = BertForSequenceClassification.from_pretrained(
            'klue/bert-base',
            num_labels=3  # ê¸ì •, ì¤‘ë¦½, ë¶€ì •
        )
        self.model.load_state_dict(torch.load('models/news_sentiment.pt'))
        
    def analyze_sentiment(self, text):
        """ë‰´ìŠ¤ ê°ì„± ë¶„ì„"""
        # í† í°í™”
        inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # ê°ì„± ì ìˆ˜ ê³„ì‚°
        sentiment_scores = {
            'positive': float(predictions[0][0]),
            'neutral': float(predictions[0][1]),
            'negative': float(predictions[0][2])
        }
        
        # ì£¼ìš” ê°ì„± ê²°ì •
        sentiment = max(sentiment_scores, key=sentiment_scores.get)
        confidence = sentiment_scores[sentiment]
        
        return {
            'sentiment': sentiment,
            'confidence': confidence,
            'scores': sentiment_scores
        }
```

### 2. ì´ì§„ ë¶„ë¥˜ ê°ì„± ë¶„ì„ (Enhanced)
```python
class BinarySentimentAnalyzer:
    """í–¥ìƒëœ ì´ì§„ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.model = self.load_fine_tuned_model()
        self.keyword_weights = self.load_keyword_weights()
        
    def analyze_binary_sentiment(self, text):
        """ê¸ì •/ë¶€ì • ì´ì§„ ë¶„ë¥˜"""
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì´ˆê¸° ë¶„ì„
        keyword_score = self.keyword_analysis(text)
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡
        model_score = self.model_prediction(text)
        
        # ì•™ìƒë¸” ê²°ê³¼
        final_score = 0.7 * model_score + 0.3 * keyword_score
        
        return {
            'sentiment': 'positive' if final_score > 0.5 else 'negative',
            'score': final_score,
            'confidence': abs(final_score - 0.5) * 2,
            'keyword_influence': keyword_score,
            'model_influence': model_score
        }
    
    def keyword_analysis(self, text):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ì ìˆ˜"""
        positive_keywords = [
            'ìƒìŠ¹', 'ê¸‰ë“±', 'ì‹ ê³ ê°€', 'í˜¸ì¬', 'ì„±ì¥', 'ê°œì„ ', 'ì¦ê°€',
            'í‘ì', 'ìˆ˜í˜œ', 'ê¸°ëŒ€', 'í˜¸ì¡°', 'ê°•ì„¸', 'ëŒíŒŒ', 'íšŒë³µ'
        ]
        negative_keywords = [
            'í•˜ë½', 'ê¸‰ë½', 'ì‹ ì €ê°€', 'ì•…ì¬', 'ìœ„ì¶•', 'ì•…í™”', 'ê°ì†Œ',
            'ì ì', 'í”¼í•´', 'ìš°ë ¤', 'ë¶€ì§„', 'ì•½ì„¸', 'ì´íƒˆ', 'ì¹¨ì²´'
        ]
        
        pos_count = sum(1 for word in positive_keywords if word in text)
        neg_count = sum(1 for word in negative_keywords if word in text)
        
        if pos_count + neg_count == 0:
            return 0.5
        
        return pos_count / (pos_count + neg_count)
```

## ë‰´ìŠ¤ ë¶„ë¥˜ ë° íƒœê¹…

### 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹œìŠ¤í…œ
```python
class NewsClassifier:
    def __init__(self):
        self.categories = {
            'market': ['ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì§€ìˆ˜', 'ì‹œí™©', 'ì¥ë§ˆê°'],
            'company': ['ì‹¤ì ', 'ê³µì‹œ', 'ì¸ìˆ˜', 'í•©ë³‘', 'M&A'],
            'sector': ['ë°˜ë„ì²´', 'ë°”ì´ì˜¤', 'ë°°í„°ë¦¬', 'ìë™ì°¨', 'IT'],
            'economy': ['ê¸ˆë¦¬', 'í™˜ìœ¨', 'GDP', 'ë¬¼ê°€', 'ë¶€ë™ì‚°'],
            'global': ['ë¯¸êµ­', 'ì¤‘êµ­', 'ìœ ëŸ½', 'Fed', 'ECB'],
            'crypto': ['ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ì•”í˜¸í™”í', 'ê°€ìƒìì‚°'],
            'commodity': ['ì›ìœ ', 'ê¸ˆ', 'êµ¬ë¦¬', 'ì›ìì¬', 'ê³¡ë¬¼']
        }
        
    def classify_news(self, title, content):
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text = title + " " + content
        
        category_scores = {}
        for category, keywords in self.categories.items():
            score = sum(1 for keyword in keywords if keyword in text)
            category_scores[category] = score
        
        # ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì„ íƒ
        primary_category = max(category_scores, key=category_scores.get)
        
        # ë©€í‹° ì¹´í…Œê³ ë¦¬ (ì ìˆ˜ > 0ì¸ ëª¨ë“  ì¹´í…Œê³ ë¦¬)
        all_categories = [cat for cat, score in category_scores.items() 
                         if score > 0]
        
        return {
            'primary': primary_category,
            'all': all_categories,
            'scores': category_scores
        }
```

### 2. ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
```python
class StockMentionExtractor:
    def __init__(self):
        self.stock_dict = self.load_stock_dictionary()
        self.company_patterns = self.compile_patterns()
        
    def extract_mentions(self, text):
        """ë‰´ìŠ¤ì—ì„œ ì¢…ëª© ì–¸ê¸‰ ì¶”ì¶œ"""
        mentioned_stocks = []
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
        ticker_pattern = r'\((\d{6})\)'
        tickers = re.findall(ticker_pattern, text)
        
        for ticker in tickers:
            if ticker in self.stock_dict:
                mentioned_stocks.append({
                    'ticker': ticker,
                    'name': self.stock_dict[ticker]['name'],
                    'market': self.stock_dict[ticker]['market']
                })
        
        # íšŒì‚¬ëª…ìœ¼ë¡œ ì¶”ì¶œ
        for company, info in self.stock_dict.items():
            if info['name'] in text:
                if not any(s['ticker'] == company for s in mentioned_stocks):
                    mentioned_stocks.append({
                        'ticker': company,
                        'name': info['name'],
                        'market': info['market']
                    })
        
        return mentioned_stocks
```

## ê°œì¸í™” ë‰´ìŠ¤ íë ˆì´ì…˜

### 1. ì‚¬ìš©ì ê´€ì‹¬ì‚¬ í•™ìŠµ
```python
class NewsPersonalization:
    def __init__(self):
        self.user_profiles = {}
        
    def learn_user_preferences(self, user_id, interactions):
        """ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµ"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = {
                'categories': defaultdict(float),
                'keywords': defaultdict(float),
                'stocks': defaultdict(float),
                'sentiment_preference': 0.5
            }
        
        profile = self.user_profiles[user_id]
        
        for interaction in interactions:
            # í´ë¦­í•œ ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜ ì¦ê°€
            if interaction['action'] == 'click':
                profile['categories'][interaction['category']] += 1.0
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê°€ì¤‘ì¹˜ ì¦ê°€
                keywords = self.extract_keywords(interaction['content'])
                for keyword in keywords:
                    profile['keywords'][keyword] += 0.5
                
                # ê´€ë ¨ ì¢…ëª© ê°€ì¤‘ì¹˜ ì¦ê°€
                for stock in interaction['related_stocks']:
                    profile['stocks'][stock] += 0.8
            
            # ì²´ë¥˜ ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            if interaction['read_time'] > 30:  # 30ì´ˆ ì´ìƒ ì½ìŒ
                weight_bonus = min(interaction['read_time'] / 60, 2.0)
                profile['categories'][interaction['category']] += weight_bonus
        
        return profile
```

### 2. ë§ì¶¤í˜• ë‰´ìŠ¤ ì¶”ì²œ
```python
class NewsRecommender:
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000)
        
    def recommend_news(self, user_profile, news_pool, top_k=20):
        """ê°œì¸í™”ëœ ë‰´ìŠ¤ ì¶”ì²œ"""
        scored_news = []
        
        for news in news_pool:
            score = 0
            
            # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì ìˆ˜
            category_score = user_profile['categories'].get(
                news['category'], 0
            ) * 0.3
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_score = sum(
                user_profile['keywords'].get(kw, 0) 
                for kw in news['keywords']
            ) * 0.3
            
            # ì¢…ëª© ê´€ë ¨ì„± ì ìˆ˜
            stock_score = sum(
                user_profile['stocks'].get(stock['ticker'], 0)
                for stock in news['related_stocks']
            ) * 0.2
            
            # ê°ì„± ì„ í˜¸ë„ ì ìˆ˜
            sentiment_match = 1 - abs(
                user_profile['sentiment_preference'] - 
                news['sentiment']['score']
            )
            sentiment_score = sentiment_match * 0.1
            
            # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœì‹  ë‰´ìŠ¤ ìš°ì„ )
            time_decay = self.calculate_time_decay(news['published'])
            time_score = time_decay * 0.1
            
            # ì´ì  ê³„ì‚°
            total_score = (category_score + keyword_score + 
                          stock_score + sentiment_score + time_score)
            
            scored_news.append({
                'news': news,
                'score': total_score,
                'breakdown': {
                    'category': category_score,
                    'keyword': keyword_score,
                    'stock': stock_score,
                    'sentiment': sentiment_score,
                    'recency': time_score
                }
            })
        
        # ìƒìœ„ Kê°œ ì¶”ì²œ
        recommended = sorted(scored_news, 
                           key=lambda x: x['score'], 
                           reverse=True)[:top_k]
        
        return recommended
```

## ë‰´ìŠ¤ ìš”ì•½ ë° ì¸ì‚¬ì´íŠ¸

### 1. AI ê¸°ë°˜ ë‰´ìŠ¤ ìš”ì•½
```python
class NewsSummarizer:
    def __init__(self):
        self.summarizer = pipeline(
            "summarization",
            model="gogamza/kobart-summarization"
        )
        
    def summarize_article(self, article_text, max_length=150):
        """ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½"""
        # ê¸´ í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = self.split_text(article_text, max_chunk_size=1024)
        
        # ê° ì²­í¬ ìš”ì•½
        summaries = []
        for chunk in chunks:
            summary = self.summarizer(
                chunk,
                max_length=max_length,
                min_length=50,
                do_sample=False
            )[0]['summary_text']
            summaries.append(summary)
        
        # ìš”ì•½ í†µí•©
        if len(summaries) > 1:
            final_summary = self.combine_summaries(summaries)
        else:
            final_summary = summaries[0]
        
        return {
            'summary': final_summary,
            'key_points': self.extract_key_points(article_text),
            'word_count': len(final_summary.split()),
            'compression_ratio': len(final_summary) / len(article_text)
        }
    
    def extract_key_points(self, text):
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        sentences = text.split('.')
        
        # TF-IDFë¡œ ì¤‘ìš” ë¬¸ì¥ ì„ íƒ
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        
        # ë¬¸ì¥ ì¤‘ìš”ë„ ê³„ì‚°
        sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]
        
        # ìƒìœ„ 3ê°œ ë¬¸ì¥ ì„ íƒ
        top_indices = sorted(range(len(sentence_scores)), 
                           key=lambda i: sentence_scores[i], 
                           reverse=True)[:3]
        
        key_points = [sentences[i].strip() for i in sorted(top_indices)]
        
        return key_points
```

### 2. ì‹œì¥ ì˜í–¥ë„ ë¶„ì„
```python
class MarketImpactAnalyzer:
    def __init__(self):
        self.impact_keywords = {
            'high': ['ê¸ˆë¦¬ ì¸ìƒ', 'ê¸°ì¤€ê¸ˆë¦¬', 'ëŒ€ê·œëª¨ ì¸ìˆ˜', 'ì •ë¶€ ê·œì œ', 
                    'ë¬´ì—­ ì „ìŸ', 'íŒ¬ë°ë¯¹', 'ì „ìŸ', 'íŒŒì‚°'],
            'medium': ['ì‹¤ì  ë°œí‘œ', 'ì‹ ì œí’ˆ ì¶œì‹œ', 'íˆ¬ì ìœ ì¹˜', 'ì—…í™© ê°œì„ ',
                      'ìˆ˜ì£¼ í™•ëŒ€', 'ë§¤ì¶œ ì¦ê°€', 'ì˜ì—…ì´ìµ'],
            'low': ['ì¸ì‚¬ ë³€ë™', 'ì‚¬ë¬´ì‹¤ ì´ì „', 'ì†Œê·œëª¨ íˆ¬ì', 'í–‰ì‚¬ ê°œìµœ']
        }
        
    def analyze_impact(self, news):
        """ë‰´ìŠ¤ì˜ ì‹œì¥ ì˜í–¥ë„ ë¶„ì„"""
        text = news['title'] + " " + news['content']
        
        # ì˜í–¥ë„ ë ˆë²¨ ê²°ì •
        impact_level = 'low'
        for level, keywords in self.impact_keywords.items():
            if any(keyword in text for keyword in keywords):
                impact_level = level
                break
        
        # ì˜í–¥ ë²”ìœ„ ë¶„ì„
        affected_sectors = self.identify_affected_sectors(text)
        affected_stocks = self.identify_affected_stocks(text)
        
        # ì˜ˆìƒ ì§€ì† ê¸°ê°„
        duration = self.estimate_impact_duration(impact_level)
        
        # ì‹œì¥ ë°˜ì‘ ì˜ˆì¸¡
        market_reaction = self.predict_market_reaction(news, impact_level)
        
        return {
            'level': impact_level,
            'score': self.calculate_impact_score(impact_level),
            'affected_sectors': affected_sectors,
            'affected_stocks': affected_stocks,
            'duration': duration,
            'market_reaction': market_reaction,
            'confidence': self.calculate_confidence(news)
        }
```

## ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ

### 1. í‚¤ì›Œë“œ ì•Œë¦¼
```python
class NewsAlertSystem:
    def __init__(self):
        self.user_alerts = defaultdict(list)
        
    def add_keyword_alert(self, user_id, keywords):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì•Œë¦¼ ì„¤ì •"""
        self.user_alerts[user_id].append({
            'type': 'keyword',
            'keywords': keywords,
            'created_at': datetime.now()
        })
    
    def add_stock_alert(self, user_id, tickers):
        """ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ì•Œë¦¼"""
        self.user_alerts[user_id].append({
            'type': 'stock',
            'tickers': tickers,
            'created_at': datetime.now()
        })
    
    def check_alerts(self, news):
        """ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        triggered_alerts = []
        
        for user_id, alerts in self.user_alerts.items():
            for alert in alerts:
                if alert['type'] == 'keyword':
                    if any(kw in news['title'] or kw in news['content'] 
                          for kw in alert['keywords']):
                        triggered_alerts.append({
                            'user_id': user_id,
                            'news': news,
                            'alert_type': 'keyword',
                            'matched': [kw for kw in alert['keywords'] 
                                       if kw in news['title'] or kw in news['content']]
                        })
                
                elif alert['type'] == 'stock':
                    mentioned_tickers = [s['ticker'] for s in news['related_stocks']]
                    if any(ticker in mentioned_tickers for ticker in alert['tickers']):
                        triggered_alerts.append({
                            'user_id': user_id,
                            'news': news,
                            'alert_type': 'stock',
                            'matched': [t for t in alert['tickers'] 
                                       if t in mentioned_tickers]
                        })
        
        return triggered_alerts
```

### 2. í‘¸ì‹œ ì•Œë¦¼ ì „ì†¡
```python
def send_push_notification(user_id, alert):
    """í‘¸ì‹œ ì•Œë¦¼ ì „ì†¡"""
    notification = {
        'title': f"ğŸ“° {alert['alert_type'].upper()} ì•Œë¦¼",
        'body': alert['news']['title'],
        'data': {
            'news_id': alert['news']['id'],
            'url': alert['news']['link'],
            'matched': alert['matched']
        }
    }
    
    # FCM ë˜ëŠ” ì›¹ í‘¸ì‹œ ì „ì†¡
    if user_has_mobile_token(user_id):
        send_fcm_notification(user_id, notification)
    elif user_has_web_subscription(user_id):
        send_web_push(user_id, notification)
    
    # ì´ë©”ì¼ ì•Œë¦¼ (ì„ íƒì )
    if user_wants_email(user_id):
        send_email_notification(user_id, alert)
```

## ë‰´ìŠ¤ ì‹œê°í™”

### 1. ê°ì„± íŠ¸ë Œë“œ ì°¨íŠ¸
```javascript
// ì‹œê°„ëŒ€ë³„ ê°ì„± íŠ¸ë Œë“œ
const sentimentTrendChart = new Chart(ctx, {
    type: 'line',
    data: {
        labels: hourlyLabels,
        datasets: [{
            label: 'ê¸ì •',
            data: positiveScores,
            borderColor: 'green',
            backgroundColor: 'rgba(0,255,0,0.1)'
        }, {
            label: 'ë¶€ì •',
            data: negativeScores,
            borderColor: 'red',
            backgroundColor: 'rgba(255,0,0,0.1)'
        }, {
            label: 'ì¤‘ë¦½',
            data: neutralScores,
            borderColor: 'gray',
            backgroundColor: 'rgba(128,128,128,0.1)'
        }]
    },
    options: {
        responsive: true,
        plugins: {
            title: {
                display: true,
                text: 'ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ê°ì„± íŠ¸ë Œë“œ'
            }
        }
    }
});
```

### 2. í‚¤ì›Œë“œ í´ë¼ìš°ë“œ
```python
def generate_keyword_cloud(news_list):
    """í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±"""
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    
    # ëª¨ë“  ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ê²°í•©
    all_text = ' '.join([n['title'] + ' ' + n['content'] 
                         for n in news_list])
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = load_korean_stopwords()
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wordcloud = WordCloud(
        font_path='NanumGothic.ttf',
        width=800,
        height=400,
        background_color='white',
        stopwords=stopwords,
        max_words=100
    ).generate(all_text)
    
    # ì´ë¯¸ì§€ ì €ì¥
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout(pad=0)
    
    return wordcloud
```

## ë‰´ìŠ¤ í˜ì´ì§€ UI/UX

### 1. ë‰´ìŠ¤ ì¹´ë“œ ì»´í¬ë„ŒíŠ¸
```html
<div class="news-card">
    <div class="news-header">
        <span class="news-source">í•œêµ­ê²½ì œ</span>
        <span class="news-time">10ë¶„ ì „</span>
        <span class="sentiment-badge positive">ê¸ì •</span>
    </div>
    
    <h3 class="news-title">
        ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì˜ì—…ì´ìµ 9.18ì¡°ì›...ì‹œì¥ ì˜ˆìƒì¹˜ ìƒíšŒ
    </h3>
    
    <p class="news-summary">
        ì‚¼ì„±ì „ìê°€ 3ë¶„ê¸° ì˜ì—…ì´ìµ 9ì¡°1800ì–µì›ì„ ê¸°ë¡í–ˆë‹¤. 
        ì´ëŠ” ì „ë…„ ë™ê¸° ëŒ€ë¹„ 277% ì¦ê°€í•œ ìˆ˜ì¹˜ë¡œ...
    </p>
    
    <div class="news-tags">
        <span class="tag">#ì‚¼ì„±ì „ì</span>
        <span class="tag">#ë°˜ë„ì²´</span>
        <span class="tag">#ì‹¤ì </span>
    </div>
    
    <div class="related-stocks">
        <span class="stock-chip">
            005930 <span class="change up">+2.3%</span>
        </span>
    </div>
    
    <div class="news-actions">
        <button class="btn-read-more">ìì„¸íˆ ë³´ê¸°</button>
        <button class="btn-save">ì €ì¥</button>
        <button class="btn-share">ê³µìœ </button>
    </div>
</div>
```

### 2. í•„í„°ë§ ë° ì •ë ¬ ì˜µì…˜
```html
<div class="news-filters">
    <select id="category-filter">
        <option value="all">ì „ì²´</option>
        <option value="market">ì‹œí™©</option>
        <option value="company">ê¸°ì—…</option>
        <option value="economy">ê²½ì œ</option>
        <option value="global">í•´ì™¸</option>
    </select>
    
    <select id="sentiment-filter">
        <option value="all">ëª¨ë“  ê°ì„±</option>
        <option value="positive">ê¸ì •</option>
        <option value="negative">ë¶€ì •</option>
        <option value="neutral">ì¤‘ë¦½</option>
    </select>
    
    <input type="text" 
           id="keyword-search" 
           placeholder="í‚¤ì›Œë“œ ê²€ìƒ‰...">
    
    <select id="sort-order">
        <option value="recent">ìµœì‹ ìˆœ</option>
        <option value="relevant">ê´€ë ¨ë„ìˆœ</option>
        <option value="impact">ì˜í–¥ë„ìˆœ</option>
    </select>
</div>
```

## API ì—”ë“œí¬ì¸íŠ¸

### 1. ë‰´ìŠ¤ ëª©ë¡ ì¡°íšŒ
```python
@app.route('/api/news', methods=['GET'])
def get_news_list():
    """ë‰´ìŠ¤ ëª©ë¡ API"""
    # íŒŒë¼ë¯¸í„° íŒŒì‹±
    category = request.args.get('category', 'all')
    sentiment = request.args.get('sentiment', 'all')
    keyword = request.args.get('keyword', '')
    page = int(request.args.get('page', 1))
    limit = int(request.args.get('limit', 20))
    
    # í•„í„°ë§
    query = build_news_query(category, sentiment, keyword)
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    total_count = count_news(query)
    news_list = fetch_news(query, page, limit)
    
    # ê°œì¸í™” (ë¡œê·¸ì¸ ì‚¬ìš©ì)
    if 'user_id' in session:
        news_list = personalize_news(session['user_id'], news_list)
    
    return jsonify({
        'news': news_list,
        'pagination': {
            'page': page,
            'limit': limit,
            'total': total_count,
            'pages': (total_count + limit - 1) // limit
        }
    })
```

### 2. ë‰´ìŠ¤ ê°ì„± ë¶„ì„ API
```python
@app.route('/api/news/sentiment/<news_id>', methods=['GET'])
def get_news_sentiment(news_id):
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼"""
    news = get_news_by_id(news_id)
    
    if not news:
        return jsonify({'error': 'News not found'}), 404
    
    # ê°ì„± ë¶„ì„ ìˆ˜í–‰
    sentiment = sentiment_analyzer.analyze(news['content'])
    
    # ê´€ë ¨ ì¢…ëª© ê°ì„± ì˜í–¥ë„
    stock_impacts = []
    for stock in news['related_stocks']:
        impact = calculate_stock_impact(stock, sentiment)
        stock_impacts.append({
            'ticker': stock['ticker'],
            'name': stock['name'],
            'impact': impact
        })
    
    return jsonify({
        'news_id': news_id,
        'sentiment': sentiment,
        'stock_impacts': stock_impacts,
        'market_impact': calculate_market_impact(sentiment),
        'confidence': sentiment['confidence']
    })
```

## ë°ì´í„° ì €ì¥ êµ¬ì¡°

### 1. ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
```sql
CREATE TABLE news (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    content TEXT,
    summary TEXT,
    source VARCHAR(100),
    author VARCHAR(100),
    published_at TIMESTAMP,
    url VARCHAR(500) UNIQUE,
    category VARCHAR(50),
    sentiment VARCHAR(20),
    sentiment_score FLOAT,
    importance_score FLOAT,
    view_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE news_stocks (
    news_id INTEGER REFERENCES news(id),
    ticker VARCHAR(10),
    company_name VARCHAR(100),
    mention_count INTEGER,
    sentiment_impact FLOAT,
    PRIMARY KEY (news_id, ticker)
);

CREATE TABLE news_keywords (
    id SERIAL PRIMARY KEY,
    news_id INTEGER REFERENCES news(id),
    keyword VARCHAR(50),
    tf_idf_score FLOAT
);

CREATE TABLE user_news_interactions (
    user_id VARCHAR(50),
    news_id INTEGER REFERENCES news(id),
    action VARCHAR(20),  -- view, click, save, share
    duration INTEGER,  -- reading time in seconds
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹± ì „ëµ
```python
# Redis ìºì‹±
cache_config = {
    'news_list': 60,  # 1ë¶„
    'sentiment_analysis': 3600,  # 1ì‹œê°„
    'keyword_cloud': 1800,  # 30ë¶„
    'trending_topics': 300  # 5ë¶„
}

def get_cached_news(category, page):
    cache_key = f"news:{category}:{page}"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    # ìºì‹œ ë¯¸ìŠ¤ - DBì—ì„œ ì¡°íšŒ
    news = fetch_news_from_db(category, page)
    
    # ìºì‹œ ì €ì¥
    redis_client.setex(
        cache_key,
        cache_config['news_list'],
        json.dumps(news)
    )
    
    return news
```

### 2. ë°°ì¹˜ ì²˜ë¦¬
```python
def batch_process_news():
    """ë‰´ìŠ¤ ë°°ì¹˜ ì²˜ë¦¬"""
    # ëŒ€ëŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘
    news_batch = collect_bulk_news()
    
    # ë²¡í„°í™” (í•œ ë²ˆì— ì²˜ë¦¬)
    texts = [n['content'] for n in news_batch]
    vectors = tfidf_vectorizer.transform(texts)
    
    # ë³‘ë ¬ ê°ì„± ë¶„ì„
    with ThreadPoolExecutor(max_workers=4) as executor:
        sentiments = list(executor.map(
            analyze_sentiment, 
            news_batch
        ))
    
    # DB ì¼ê´„ ì‚½ì…
    bulk_insert_news(news_batch, sentiments, vectors)
```

## í–¥í›„ ê°œë°œ ê³„íš

1. **AI ê³ ë„í™”**
   - GPT-4 ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„
   - ë‹¤êµ­ì–´ ë‰´ìŠ¤ ì§€ì›
   - ê°€ì§œ ë‰´ìŠ¤ íƒì§€

2. **ì‹¤ì‹œê°„ ê¸°ëŠ¥ ê°•í™”**
   - WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ í”¼ë“œ
   - ë¼ì´ë¸Œ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë°
   - ì†ë³´ ì•Œë¦¼ ì‹œìŠ¤í…œ

3. **ì†Œì…œ ê¸°ëŠ¥**
   - ë‰´ìŠ¤ ëŒ“ê¸€ ì‹œìŠ¤í…œ
   - ì‚¬ìš©ì ê°„ ë‰´ìŠ¤ ê³µìœ 
   - ì „ë¬¸ê°€ ì˜ê²¬ í†µí•©

4. **ë¶„ì„ ë„êµ¬**
   - ë‰´ìŠ¤ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸
   - ì´ë²¤íŠ¸ ì„íŒ©íŠ¸ ë¶„ì„
   - ì„¼í‹°ë¨¼íŠ¸ ëŒ€ì‹œë³´ë“œ

5. **ê°œì¸í™” ê°•í™”**
   - ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ
   - ì½ê¸° íŒ¨í„´ í•™ìŠµ
   - ë§ì¶¤í˜• ìš”ì•½ ìƒì„±