# 05. 사용 데이터

## 5.1 데이터 수집 전략

### 5.1.1 데이터 소스 및 수집 체계

Pixie 서비스는 정확하고 신뢰할 수 있는 투자 조언을 제공하기 위해 다양한 소스로부터 고품질 금융 데이터를 수집합니다. 데이터 수집은 자동화된 파이프라인을 통해 이루어지며, 실시간성과 정확성을 보장하기 위한 다층적 검증 시스템을 운영합니다.

**주요 데이터 소스**

1. **한국거래소 (KRX)**
   - 실시간 주가 데이터
   - 거래량 및 거래대금
   - 투자자별 매매 동향
   - 공시 정보 및 기업 공고

2. **금융감독원 전자공시시스템 (DART)**
   - 기업 재무제표 (분기/반기/연간)
   - 사업보고서 및 감사보고서
   - 주요사항보고서
   - 지분 변동 정보

3. **Yahoo Finance API**
   - 미국 주식 실시간 가격
   - 글로벌 지수 데이터
   - 환율 정보
   - 원자재 가격

4. **뉴스 및 미디어 소스**
   - 주요 경제 신문 RSS 피드 (한국경제, 매일경제, 조선비즈 등)
   - 금융 전문 미디어 (이데일리, 머니투데이, 서울경제 등)
   - 글로벌 금융 뉴스 (Bloomberg, Reuters, CNBC)
   - 소셜 미디어 트렌드 (Twitter, Reddit의 투자 관련 포스트)

5. **대안 데이터 소스**
   - 위성 이미지 데이터 (주차장 차량 수, 공장 가동률 등)
   - 신용카드 소비 트렌드
   - 앱 다운로드 및 사용량 통계
   - 검색 트렌드 데이터

### 5.1.2 데이터 수집 주기 및 볼륨

**실시간 수집 (1초-1분 간격)**
- 주가 및 호가 데이터: 200개 한국 종목 + 20개 미국 종목
- 거래량 데이터: 분당 약 10,000건의 거래 데이터
- 지수 데이터: KOSPI, KOSDAQ, S&P 500, NASDAQ 등 주요 10개 지수
- 일일 처리량: 약 500만 개의 가격 포인트

**준실시간 수집 (5분-1시간 간격)**
- 뉴스 기사: 시간당 평균 200개 기사
- 공시 정보: 일평균 50-100건
- 환율 및 원자재 가격: 15분 간격
- 일일 처리량: 약 5,000개의 뉴스 및 공시 문서

**일일 수집**
- 재무제표 업데이트: 분기별 약 2,000개 기업
- 투자자별 매매 동향: 전체 상장사 데이터
- 기술적 지표 계산: 200개 종목 × 20개 지표
- 일일 처리량: 약 50MB의 구조화된 재무 데이터

**주간/월간 수집**
- 애널리스트 리포트: 주간 평균 50개
- 산업 동향 보고서: 월간 10-20개
- 경제 지표: GDP, 실업률, 금리 등
- 월간 처리량: 약 1GB의 리서치 데이터

## 5.2 데이터 전처리 방식

### 5.2.1 데이터 정제 및 표준화

**데이터 품질 관리 파이프라인**

1. **수집 단계 검증**
   ```python
   # 데이터 수집 시 기본 검증
   def validate_raw_data(data):
       # NULL 값 체크
       if data.isnull().sum() > threshold:
           log_warning("과도한 결측치 발견")
       
       # 이상치 탐지 (IQR 방법)
       Q1 = data.quantile(0.25)
       Q3 = data.quantile(0.75)
       IQR = Q3 - Q1
       outliers = (data < Q1 - 1.5 * IQR) | (data > Q3 + 1.5 * IQR)
       
       # 시계열 연속성 체크
       time_gaps = check_time_continuity(data)
       if time_gaps:
           interpolate_missing_points(data, time_gaps)
       
       return cleaned_data
   ```

2. **데이터 정규화**
   - 가격 데이터: 액면분할, 배당 조정
   - 재무 데이터: 회계 기준 통일 (K-IFRS)
   - 시계열 데이터: 시간대 표준화 (KST 기준)
   - 텍스트 데이터: 인코딩 통일 (UTF-8)

3. **결측치 처리**
   - 시계열 데이터: 선형 보간법 또는 이전 값 대체
   - 재무 데이터: 산업 평균값 또는 회귀 추정
   - 범주형 데이터: 최빈값 대체 또는 별도 카테고리 생성

### 5.2.2 특징 추출 (Feature Engineering)

**기술적 지표 계산**

1. **추세 지표**
   - 이동평균선 (SMA, EMA): 5일, 20일, 60일, 120일
   - MACD (Moving Average Convergence Divergence)
   - 볼린저 밴드 (Bollinger Bands)
   - 일목균형표 (Ichimoku Cloud)

2. **모멘텀 지표**
   - RSI (Relative Strength Index)
   - 스토캐스틱 (Stochastic Oscillator)
   - CCI (Commodity Channel Index)
   - Williams %R

3. **거래량 지표**
   - OBV (On-Balance Volume)
   - 거래량 이동평균
   - VWAP (Volume Weighted Average Price)
   - 체이킨 머니 플로우 (Chaikin Money Flow)

**펀더멘탈 지표 계산**

1. **가치 평가 지표**
   - PER (Price to Earnings Ratio)
   - PBR (Price to Book Ratio)
   - PSR (Price to Sales Ratio)
   - EV/EBITDA

2. **수익성 지표**
   - ROE (Return on Equity)
   - ROA (Return on Assets)
   - 영업이익률
   - 순이익률

3. **성장성 지표**
   - 매출 성장률 (YoY, QoQ)
   - 이익 성장률
   - PEG (Price/Earnings to Growth)

### 5.2.3 텍스트 데이터 처리

**뉴스 감성 분석 파이프라인**

1. **텍스트 전처리**
   ```python
   def preprocess_news_text(text):
       # HTML 태그 제거
       text = remove_html_tags(text)
       
       # 특수문자 및 이모지 제거
       text = remove_special_chars(text)
       
       # 한국어 형태소 분석 (Mecab 사용)
       tokens = mecab.morphs(text)
       
       # 불용어 제거
       tokens = [t for t in tokens if t not in stopwords]
       
       # 명사 추출
       nouns = [t for t in tokens if t[1] in ['NNG', 'NNP']]
       
       return processed_text
   ```

2. **감성 점수 계산**
   - 사전 기반 감성 분석 (긍정/부정 단어 사전)
   - BERT 기반 감성 분류 모델
   - 뉴스 제목과 본문 가중치 차별화
   - -1(매우 부정) ~ +1(매우 긍정) 스케일

3. **주제 모델링**
   - LDA (Latent Dirichlet Allocation)를 통한 주제 추출
   - 주요 키워드 추출 (TF-IDF)
   - 기업명 및 티커 태깅
   - 산업/섹터 분류

## 5.3 데이터 저장 및 관리

### 5.3.1 데이터베이스 구조

**관계형 데이터베이스 (PostgreSQL)**

1. **주가 데이터 테이블**
   ```sql
   CREATE TABLE stock_prices (
       id BIGSERIAL PRIMARY KEY,
       ticker VARCHAR(20) NOT NULL,
       date DATE NOT NULL,
       open DECIMAL(10,2),
       high DECIMAL(10,2),
       low DECIMAL(10,2),
       close DECIMAL(10,2),
       volume BIGINT,
       adjusted_close DECIMAL(10,2),
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       UNIQUE(ticker, date)
   );
   
   CREATE INDEX idx_ticker_date ON stock_prices(ticker, date);
   ```

2. **재무제표 테이블**
   ```sql
   CREATE TABLE financial_statements (
       id BIGSERIAL PRIMARY KEY,
       ticker VARCHAR(20) NOT NULL,
       period_end DATE NOT NULL,
       report_type VARCHAR(20), -- 'quarterly', 'annual'
       revenue DECIMAL(15,2),
       operating_income DECIMAL(15,2),
       net_income DECIMAL(15,2),
       total_assets DECIMAL(15,2),
       total_equity DECIMAL(15,2),
       -- 추가 재무 항목들
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```

3. **뉴스 데이터 테이블**
   ```sql
   CREATE TABLE news_articles (
       id BIGSERIAL PRIMARY KEY,
       title TEXT NOT NULL,
       content TEXT,
       source VARCHAR(100),
       published_at TIMESTAMP,
       sentiment_score DECIMAL(3,2),
       related_tickers TEXT[], -- PostgreSQL 배열 타입
       keywords TEXT[],
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```

**NoSQL 데이터베이스 (MongoDB)**

실시간 스트리밍 데이터와 비정형 데이터 저장:

```javascript
// 실시간 거래 데이터
{
  "_id": ObjectId("..."),
  "ticker": "005930",
  "timestamp": ISODate("2024-01-15T09:30:00Z"),
  "price": 75000,
  "volume": 1000,
  "bid_ask_spread": {
    "bid": 74900,
    "ask": 75100
  },
  "order_book": [
    {"price": 75100, "volume": 5000, "side": "ask"},
    {"price": 75000, "volume": 3000, "side": "ask"},
    // ...
  ]
}
```

**시계열 데이터베이스 (InfluxDB)**

고성능 시계열 데이터 처리:

```sql
-- 실시간 가격 데이터 저장
INSERT stock_prices,ticker=005930 
  price=75000,volume=1000,bid=74900,ask=75100 
  1642233000000000000
```

### 5.3.2 데이터 캐싱 전략

**Redis 캐싱 레이어**

1. **실시간 데이터 캐싱**
   - 최근 가격 데이터: 1분 TTL
   - 호가 정보: 10초 TTL
   - 일중 거래량: 5분 TTL

2. **계산된 지표 캐싱**
   - 기술적 지표: 1시간 TTL
   - 재무 비율: 24시간 TTL
   - 포트폴리오 수익률: 5분 TTL

3. **세션 데이터 캐싱**
   - 사용자 프로필: 세션 기간
   - AI 대화 컨텍스트: 30분 TTL
   - 검색 결과: 10분 TTL

## 5.4 데이터 품질 및 거버넌스

### 5.4.1 데이터 품질 메트릭

**품질 지표 모니터링**

1. **완전성 (Completeness)**
   - 목표: 95% 이상의 데이터 완전성
   - 측정: 필수 필드의 NULL 값 비율
   - 알림: 90% 미만 시 자동 알림

2. **정확성 (Accuracy)**
   - 목표: 99.9% 이상의 가격 데이터 정확성
   - 측정: 복수 소스 간 교차 검증
   - 알림: 불일치 발견 시 즉시 알림

3. **일관성 (Consistency)**
   - 목표: 100% 참조 무결성 유지
   - 측정: 외래 키 제약 조건 검증
   - 알림: 무결성 위반 시 자동 복구

4. **적시성 (Timeliness)**
   - 목표: 실시간 데이터 1초 이내 반영
   - 측정: 데이터 수집-저장 시간 차이
   - 알림: 5초 이상 지연 시 알림

### 5.4.2 데이터 보안 및 개인정보 보호

**데이터 암호화**

1. **저장 시 암호화 (Encryption at Rest)**
   - AES-256 암호화 적용
   - 데이터베이스 레벨 TDE (Transparent Data Encryption)
   - 파일 시스템 암호화

2. **전송 중 암호화 (Encryption in Transit)**
   - TLS 1.3 프로토콜 사용
   - 인증서 피닝 (Certificate Pinning)
   - VPN 터널링 (민감 데이터)

**개인정보 처리**

1. **가명화 및 익명화**
   ```python
   def anonymize_user_data(user_data):
       # 개인 식별 정보 해싱
       user_data['user_id'] = hash_function(user_data['email'])
       
       # 민감 정보 마스킹
       user_data['phone'] = mask_phone_number(user_data['phone'])
       
       # 연령대로 변환
       user_data['age_group'] = get_age_group(user_data['birth_date'])
       del user_data['birth_date']
       
       return anonymized_data
   ```

2. **접근 제어**
   - 역할 기반 접근 제어 (RBAC)
   - 최소 권한 원칙
   - 감사 로그 기록
   - 정기적 권한 검토

### 5.4.3 데이터 백업 및 복구

**백업 전략**

1. **실시간 복제**
   - Master-Slave 복제 구성
   - 실시간 트랜잭션 로그 전송
   - 자동 페일오버 설정

2. **정기 백업**
   - 일일 전체 백업 (새벽 2시)
   - 시간별 증분 백업
   - 주간 백업 오프사이트 보관

3. **복구 절차**
   - RPO (Recovery Point Objective): 1시간
   - RTO (Recovery Time Objective): 4시간
   - 정기적 복구 테스트 (월 1회)

## 5.5 데이터 활용 및 분석

### 5.5.1 실시간 데이터 스트리밍

**Apache Kafka 기반 스트리밍 아키텍처**

1. **데이터 수집 레이어**
   - Producer: 각 데이터 소스별 수집기
   - Topic: 데이터 타입별 토픽 구성
   - Partition: 처리량에 따른 파티션 설정

2. **스트림 처리**
   ```python
   # Kafka Streams를 이용한 실시간 처리
   def process_price_stream(record):
       ticker = record.key
       price_data = record.value
       
       # 이동평균 계산
       moving_avg = calculate_moving_average(ticker, price_data)
       
       # 이상 거래 탐지
       if detect_anomaly(price_data):
           send_alert(ticker, price_data)
       
       # 결과 저장
       save_to_cache(ticker, processed_data)
   ```

3. **Consumer 애플리케이션**
   - 실시간 대시보드 업데이트
   - 알림 서비스
   - AI 모델 입력 데이터 준비

### 5.5.2 배치 데이터 처리

**일일 배치 작업 스케줄**

1. **새벽 2시: 데이터 정리 및 백업**
   - 이전일 데이터 아카이빙
   - 데이터베이스 인덱스 재구성
   - 전체 백업 수행

2. **새벽 4시: 해외 시장 데이터 수집**
   - 미국 시장 마감 데이터
   - 환율 및 원자재 가격
   - 글로벌 지수 업데이트

3. **오전 6시: 한국 시장 데이터 준비**
   - 전일 거래 데이터 집계
   - 투자자별 매매 동향 분석
   - 기술적 지표 사전 계산

4. **오전 7시: AI 모델 업데이트**
   - 예측 모델 재학습
   - 특징 중요도 재계산
   - 백테스팅 수행

### 5.5.3 데이터 시각화 및 리포팅

**대시보드 데이터 준비**

1. **집계 데이터 생성**
   ```sql
   -- 일일 수익률 집계
   CREATE MATERIALIZED VIEW daily_returns AS
   SELECT 
       ticker,
       date,
       (close - LAG(close) OVER (PARTITION BY ticker ORDER BY date)) 
       / LAG(close) OVER (PARTITION BY ticker ORDER BY date) * 100 as return_pct
   FROM stock_prices;
   
   -- 섹터별 성과 집계
   CREATE MATERIALIZED VIEW sector_performance AS
   SELECT 
       s.sector,
       AVG(p.return_pct) as avg_return,
       STDDEV(p.return_pct) as volatility
   FROM daily_returns p
   JOIN stock_info s ON p.ticker = s.ticker
   GROUP BY s.sector;
   ```

2. **차트 데이터 최적화**
   - 데이터 포인트 샘플링 (대용량 시계열)
   - 사전 계산된 집계값 사용
   - 클라이언트 사이드 캐싱

이러한 체계적인 데이터 수집, 처리, 저장 시스템을 통해 Pixie는 신뢰할 수 있는 고품질 데이터를 기반으로 정확한 투자 조언을 제공할 수 있습니다. 실시간 데이터 처리 능력과 대규모 히스토리컬 데이터 분석 능력을 결합하여, 사용자에게 가장 적절한 투자 인사이트를 제공합니다.