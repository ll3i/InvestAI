# ğŸ“° Pixie ë‰´ìŠ¤/ì´ìŠˆ í˜ì´ì§€ ê°œë°œ ë¬¸ì„œ

## ğŸ¯ í˜ì´ì§€ ê°œìš”

### ëª©ì 
ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘, ë¶„ì„í•˜ê³  íˆ¬ììì—ê²Œ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¢…í•© ë‰´ìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„
- AI ê¸°ë°˜ ê°ì • ë¶„ì„
- ê°œì¸í™”ëœ ë‰´ìŠ¤ í•„í„°ë§
- íˆ¬ì ì˜í–¥ë„ í‰ê°€
- ë‰´ìŠ¤ ê¸°ë°˜ íˆ¬ì ì‹ í˜¸ ìƒì„±

## ğŸ—ï¸ ê¸°ìˆ ì  ì•„í‚¤í…ì²˜

### í”„ë¡ íŠ¸ì—”ë“œ êµ¬ì¡°
```
web/templates/
â”œâ”€â”€ news.html                    # ë©”ì¸ ë‰´ìŠ¤ í˜ì´ì§€
â”œâ”€â”€ news_enhanced.html           # í–¥ìƒëœ ë‰´ìŠ¤ í˜ì´ì§€
â””â”€â”€ news_detail.html             # ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€
```

### ë°±ì—”ë“œ êµ¬ì¡°
```
web/
â”œâ”€â”€ app.py                       # ë‰´ìŠ¤ ê´€ë ¨ ë¼ìš°íŠ¸
â”œâ”€â”€ blueprints/
â”‚   â””â”€â”€ news/                   # ë‰´ìŠ¤ ê´€ë ¨ ë¼ìš°íŠ¸
â””â”€â”€ services/
    â”œâ”€â”€ news_collection_service.py # ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤
    â”œâ”€â”€ sentiment_analysis_service.py # ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤
    â””â”€â”€ news_impact_service.py   # ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„
```

## ğŸ“± UI/UX ì„¤ê³„

### ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ
1. **í—¤ë“œë¼ì¸ ì˜ì—­**
   - ì£¼ìš” ë‰´ìŠ¤ í•˜ì´ë¼ì´íŠ¸
   - ê¸´ê¸‰ ì•Œë¦¼ ë°°ë„ˆ
   - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í‘œì‹œ

2. **ì¹´í…Œê³ ë¦¬ í•„í„°**
   - ì„¹í„°ë³„ ë¶„ë¥˜
   - ì¤‘ìš”ë„ë³„ ì •ë ¬
   - ì‹œê°„ëŒ€ë³„ í•„í„°

3. **ë‰´ìŠ¤ ì¹´ë“œ ë ˆì´ì•„ì›ƒ**
   - ì œëª© ë° ìš”ì•½
   - ê°ì • ë¶„ì„ ê²°ê³¼
   - ì˜í–¥ë„ ì ìˆ˜
   - ê´€ë ¨ ì¢…ëª© íƒœê·¸

### ë°˜ì‘í˜• ë‰´ìŠ¤ ë””ìì¸
```css
/* web/static/css/news.css */
.news-container {
    display: grid;
    grid-template-columns: 1fr 350px;
    gap: 20px;
    padding: 20px;
    max-width: 1400px;
    margin: 0 auto;
}

.news-header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 20px;
    border-radius: 12px;
    margin-bottom: 20px;
}

.breaking-news {
    background: #dc3545;
    color: white;
    padding: 10px 20px;
    border-radius: 8px;
    margin-bottom: 15px;
    animation: pulse 2s infinite;
}

@keyframes pulse {
    0% { opacity: 1; }
    50% { opacity: 0.7; }
    100% { opacity: 1; }
}

.news-card {
    background: white;
    border-radius: 12px;
    padding: 20px;
    margin-bottom: 15px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    transition: transform 0.3s ease;
    border-left: 4px solid #667eea;
}

.news-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 20px rgba(0,0,0,0.15);
}

.news-title {
    font-size: 1.2rem;
    font-weight: bold;
    margin-bottom: 10px;
    color: #333;
}

.news-summary {
    color: #666;
    line-height: 1.5;
    margin-bottom: 15px;
}

.news-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    font-size: 0.9rem;
    color: #888;
}

.sentiment-indicator {
    display: flex;
    align-items: center;
    gap: 5px;
}

.sentiment-positive {
    color: #28a745;
}

.sentiment-negative {
    color: #dc3545;
}

.sentiment-neutral {
    color: #ffc107;
}

.impact-score {
    background: #f8f9fa;
    padding: 5px 10px;
    border-radius: 15px;
    font-weight: bold;
}

.related-stocks {
    display: flex;
    gap: 5px;
    flex-wrap: wrap;
    margin-top: 10px;
}

.stock-tag {
    background: #e9ecef;
    padding: 3px 8px;
    border-radius: 12px;
    font-size: 0.8rem;
    color: #495057;
}

.news-sidebar {
    background: white;
    border-radius: 12px;
    padding: 20px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    height: fit-content;
}

.category-filter {
    margin-bottom: 20px;
}

.filter-button {
    background: #f8f9fa;
    border: 1px solid #dee2e6;
    padding: 8px 15px;
    border-radius: 20px;
    margin: 2px;
    cursor: pointer;
    transition: all 0.3s ease;
}

.filter-button.active {
    background: #667eea;
    color: white;
    border-color: #667eea;
}

/* ëª¨ë°”ì¼ ìµœì í™” */
@media (max-width: 768px) {
    .news-container {
        grid-template-columns: 1fr;
    }
    
    .news-card {
        padding: 15px;
    }
    
    .news-title {
        font-size: 1.1rem;
    }
}
```

## ğŸ”§ í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„

### 1. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì„œë¹„ìŠ¤
```python
# web/services/news_collection_service.py
class NewsCollectionService:
    def __init__(self):
        self.api_clients = {
            'naver': NaverNewsAPI(),
            'yahoo': YahooFinanceAPI(),
            'reuters': ReutersAPI()
        }
        self.processor = NewsDataProcessor()
        self.cache = redis.Redis(host='localhost', port=6379, db=0)
    
    def collect_real_time_news(self) -> List[Dict[str, Any]]:
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            all_news = []
            
            for source, client in self.api_clients.items():
                try:
                    news_data = client.get_latest_news()
                    processed_news = self.processor.process_news(news_data, source)
                    all_news.extend(processed_news)
                except Exception as e:
                    logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({source}): {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_news = self.remove_duplicates(all_news)
            sorted_news = self.sort_by_relevance(unique_news)
            
            # ìºì‹œ ì €ì¥
            self.cache_news_data(sorted_news)
            
            return sorted_news
            
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def process_news(self, raw_news: List[Dict[str, Any]], source: str) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬"""
        processed_news = []
        
        for news in raw_news:
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                processed_item = {
                    'title': self.clean_text(news.get('title', '')),
                    'content': self.clean_text(news.get('content', '')),
                    'url': news.get('url', ''),
                    'source': source,
                    'published_at': self.parse_datetime(news.get('published_at')),
                    'category': self.categorize_news(news.get('title', '')),
                    'keywords': self.extract_keywords(news.get('title', '') + ' ' + news.get('content', '')),
                    'related_stocks': self.extract_stock_symbols(news.get('title', '') + ' ' + news.get('content', ''))
                }
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if self.validate_news_item(processed_item):
                    processed_news.append(processed_item)
                    
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return processed_news
    
    def categorize_news(self, title: str) -> str:
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        categories = {
            'earnings': ['ì‹¤ì ', 'ìˆ˜ìµ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ'],
            'merger': ['ì¸ìˆ˜', 'í•©ë³‘', 'M&A', 'ë§¤ê°', 'íˆ¬ì'],
            'regulation': ['ê·œì œ', 'ë²•ì•ˆ', 'ì •ì±…', 'ê°ë…', 'ì œì¬'],
            'market': ['ì‹œì¥', 'ê±°ë˜', 'ì£¼ê°€', 'ì§€ìˆ˜', 'ë™í–¥'],
            'technology': ['ê¸°ìˆ ', 'í˜ì‹ ', 'ê°œë°œ', 'íŠ¹í—ˆ', 'AI'],
            'economy': ['ê²½ì œ', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'GDP', 'ê³ ìš©']
        }
        
        title_lower = title.lower()
        
        for category, keywords in categories.items():
            if any(keyword in title_lower for keyword in keywords):
                return category
        
        return 'general'
    
    def extract_stock_symbols(self, text: str) -> List[str]:
        """ë‰´ìŠ¤ì—ì„œ ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ"""
        # í•œêµ­ ì£¼ì‹ ì¢…ëª© ì½”ë“œ íŒ¨í„´ (6ìë¦¬ ìˆ«ì)
        stock_pattern = r'\b\d{6}\b'
        stock_codes = re.findall(stock_pattern, text)
        
        # ì¢…ëª©ëª… ë§¤ì¹­
        stock_names = self.get_stock_name_matches(text)
        
        return list(set(stock_codes + stock_names))
    
    def extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        from konlpy.tag import Okt
        
        okt = Okt()
        nouns = okt.nouns(text)
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì„ íƒ
        from collections import Counter
        word_count = Counter(nouns)
        
        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        return [word for word, count in word_count.most_common(10) if len(word) > 1]
```

### 2. ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤
```python
# web/services/sentiment_analysis_service.py
class SentimentAnalysisService:
    def __init__(self):
        self.model = self.load_sentiment_model()
        self.keywords = self.load_sentiment_keywords()
    
    def analyze_news_sentiment(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ê°ì • ë¶„ì„"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = f"{news_data['title']} {news_data['content']}"
            processed_text = self.preprocess_text(text)
            
            # ê°ì • ë¶„ì„ ìˆ˜í–‰
            sentiment_score = self.model.predict_sentiment(processed_text)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
            keyword_sentiment = self.analyze_keyword_sentiment(processed_text)
            
            # ì¢…í•© ê°ì • ì ìˆ˜ ê³„ì‚°
            overall_sentiment = self.calculate_overall_sentiment(sentiment_score, keyword_sentiment)
            
            # ê°ì • ë ˆì´ë¸” ê²°ì •
            sentiment_label = self.determine_sentiment_label(overall_sentiment)
            
            return {
                'sentiment_score': overall_sentiment,
                'sentiment_label': sentiment_label,
                'confidence': self.calculate_confidence(sentiment_score, keyword_sentiment),
                'positive_keywords': keyword_sentiment['positive'],
                'negative_keywords': keyword_sentiment['negative'],
                'neutral_keywords': keyword_sentiment['neutral']
            }
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'sentiment_score': 0,
                'sentiment_label': 'neutral',
                'confidence': 0
            }
    
    def analyze_keyword_sentiment(self, text: str) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„"""
        positive_keywords = []
        negative_keywords = []
        neutral_keywords = []
        
        words = text.split()
        
        for word in words:
            if word in self.keywords['positive']:
                positive_keywords.append(word)
            elif word in self.keywords['negative']:
                negative_keywords.append(word)
            else:
                neutral_keywords.append(word)
        
        return {
            'positive': positive_keywords,
            'negative': negative_keywords,
            'neutral': neutral_keywords,
            'positive_count': len(positive_keywords),
            'negative_count': len(negative_keywords)
        }
    
    def calculate_overall_sentiment(self, model_score: float, keyword_sentiment: Dict[str, Any]) -> float:
        """ì¢…í•© ê°ì • ì ìˆ˜ ê³„ì‚°"""
        # ëª¨ë¸ ì ìˆ˜ (0-1)
        model_weight = 0.7
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
        positive_count = keyword_sentiment['positive_count']
        negative_count = keyword_sentiment['negative_count']
        total_keywords = positive_count + negative_count
        
        if total_keywords == 0:
            keyword_score = 0.5  # ì¤‘ë¦½
        else:
            keyword_score = positive_count / total_keywords
        
        keyword_weight = 0.3
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        overall_score = (model_score * model_weight) + (keyword_score * keyword_weight)
        
        return round(overall_score, 3)
    
    def determine_sentiment_label(self, score: float) -> str:
        """ê°ì • ë ˆì´ë¸” ê²°ì •"""
        if score >= 0.6:
            return 'positive'
        elif score <= 0.4:
            return 'negative'
        else:
            return 'neutral'
```

### 3. ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„ ì„œë¹„ìŠ¤
```python
# web/services/news_impact_service.py
class NewsImpactService:
    def __init__(self):
        self.stock_data_service = StockDataService()
        self.sentiment_service = SentimentAnalysisService()
    
    def analyze_news_impact(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„"""
        try:
            # ê°ì • ë¶„ì„
            sentiment_result = self.sentiment_service.analyze_news_sentiment(news_data)
            
            # ê´€ë ¨ ì¢…ëª© ë¶„ì„
            related_stocks = news_data.get('related_stocks', [])
            stock_impacts = {}
            
            for stock in related_stocks:
                stock_impact = self.analyze_stock_impact(stock, news_data, sentiment_result)
                stock_impacts[stock] = stock_impact
            
            # ì „ì²´ ì˜í–¥ë„ ê³„ì‚°
            overall_impact = self.calculate_overall_impact(stock_impacts, sentiment_result)
            
            # íˆ¬ì ì‹ í˜¸ ìƒì„±
            investment_signal = self.generate_investment_signal(overall_impact, sentiment_result)
            
            return {
                'overall_impact': overall_impact,
                'stock_impacts': stock_impacts,
                'sentiment_analysis': sentiment_result,
                'investment_signal': investment_signal,
                'risk_level': self.assess_risk_level(overall_impact),
                'confidence_score': self.calculate_confidence_score(stock_impacts, sentiment_result)
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def analyze_stock_impact(self, stock_code: str, news_data: Dict[str, Any], 
                           sentiment_result: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ì¢…ëª© ì˜í–¥ë„ ë¶„ì„"""
        try:
            # ì¢…ëª© ê¸°ë³¸ ì •ë³´
            stock_info = self.stock_data_service.get_stock_info(stock_code)
            
            # ë‰´ìŠ¤ ê´€ë ¨ì„± ì ìˆ˜
            relevance_score = self.calculate_relevance_score(stock_code, news_data)
            
            # ê°ì • ì˜í–¥ë„
            sentiment_impact = self.calculate_sentiment_impact(sentiment_result)
            
            # ì‹œì¥ ìƒí™© ê³ ë ¤
            market_context = self.get_market_context(stock_code)
            
            # ì¢…í•© ì˜í–¥ë„ ê³„ì‚°
            total_impact = (relevance_score * 0.4 + 
                          sentiment_impact * 0.4 + 
                          market_context * 0.2)
            
            return {
                'stock_code': stock_code,
                'stock_name': stock_info.get('name', ''),
                'relevance_score': relevance_score,
                'sentiment_impact': sentiment_impact,
                'market_context': market_context,
                'total_impact': total_impact,
                'expected_price_change': self.predict_price_change(total_impact, sentiment_result),
                'confidence': self.calculate_stock_confidence(relevance_score, sentiment_impact)
            }
            
        except Exception as e:
            logger.error(f"ì¢…ëª© ì˜í–¥ë„ ë¶„ì„ ì‹¤íŒ¨ ({stock_code}): {e}")
            return {}
    
    def calculate_relevance_score(self, stock_code: str, news_data: Dict[str, Any]) -> float:
        """ë‰´ìŠ¤ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        text = f"{news_data['title']} {news_data['content']}"
        
        # ì¢…ëª©ëª…/ì½”ë“œ ì¶œí˜„ ë¹ˆë„
        stock_name = self.get_stock_name(stock_code)
        stock_mentions = text.count(stock_code) + text.count(stock_name)
        
        # í‚¤ì›Œë“œ ê´€ë ¨ì„±
        keywords = news_data.get('keywords', [])
        keyword_relevance = sum(1 for keyword in keywords if self.is_stock_related(keyword, stock_code))
        
        # ìœ„ì¹˜ ê°€ì¤‘ì¹˜ (ì œëª©ì— ì–¸ê¸‰ëœ ê²½ìš° ë” ë†’ì€ ì ìˆ˜)
        title_mentions = news_data['title'].count(stock_code) + news_data['title'].count(stock_name)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (0-1)
        total_score = (stock_mentions * 0.5 + keyword_relevance * 0.3 + title_mentions * 0.2)
        normalized_score = min(total_score / 10, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì •ê·œí™”
        
        return round(normalized_score, 3)
    
    def calculate_sentiment_impact(self, sentiment_result: Dict[str, Any]) -> float:
        """ê°ì • ì˜í–¥ë„ ê³„ì‚°"""
        sentiment_score = sentiment_result.get('sentiment_score', 0.5)
        confidence = sentiment_result.get('confidence', 0.5)
        
        # ê°ì • ì ìˆ˜ë¥¼ -1ì—ì„œ 1ë¡œ ë³€í™˜
        normalized_score = (sentiment_score - 0.5) * 2
        
        # ì‹ ë¢°ë„ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
        impact_score = normalized_score * confidence
        
        return round(impact_score, 3)
    
    def predict_price_change(self, impact_score: float, sentiment_result: Dict[str, Any]) -> float:
        """ì˜ˆìƒ ê°€ê²© ë³€ë™ë¥  ê³„ì‚°"""
        # ì˜í–¥ë„ ì ìˆ˜ë¥¼ ê°€ê²© ë³€ë™ë¥ ë¡œ ë³€í™˜ (-10% ~ +10%)
        base_change = impact_score * 10
        
        # ê°ì • ê°•ë„ì— ë”°ë¥¸ ì¡°ì •
        sentiment_label = sentiment_result.get('sentiment_label', 'neutral')
        if sentiment_label == 'positive':
            multiplier = 1.2
        elif sentiment_label == 'negative':
            multiplier = 1.2
        else:
            multiplier = 0.8
        
        predicted_change = base_change * multiplier
        
        return round(predicted_change, 2)
    
    def generate_investment_signal(self, overall_impact: float, 
                                 sentiment_result: Dict[str, Any]) -> Dict[str, Any]:
        """íˆ¬ì ì‹ í˜¸ ìƒì„±"""
        sentiment_label = sentiment_result.get('sentiment_label', 'neutral')
        impact_threshold = 0.3
        
        if abs(overall_impact) < impact_threshold:
            signal = 'HOLD'
            reason = 'ë‰´ìŠ¤ ì˜í–¥ë„ê°€ ë‚®ìŠµë‹ˆë‹¤'
        elif overall_impact > impact_threshold and sentiment_label == 'positive':
            signal = 'BUY'
            reason = 'ê¸ì •ì ì¸ ë‰´ìŠ¤ ì˜í–¥'
        elif overall_impact < -impact_threshold and sentiment_label == 'negative':
            signal = 'SELL'
            reason = 'ë¶€ì •ì ì¸ ë‰´ìŠ¤ ì˜í–¥'
        else:
            signal = 'HOLD'
            reason = 'í˜¼ì¡°ì„¸ ë˜ëŠ” ì¤‘ë¦½ì  ì˜í–¥'
        
        return {
            'signal': signal,
            'reason': reason,
            'impact_score': overall_impact,
            'sentiment': sentiment_label,
            'confidence': sentiment_result.get('confidence', 0)
        }
```

## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°

### ë‰´ìŠ¤ ê´€ë ¨ í…Œì´ë¸”
```sql
CREATE TABLE news_articles (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    url VARCHAR(500) UNIQUE NOT NULL,
    source VARCHAR(50) NOT NULL,
    category VARCHAR(50) NOT NULL,
    published_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE news_sentiment (
    id SERIAL PRIMARY KEY,
    news_id INTEGER REFERENCES news_articles(id),
    sentiment_score DECIMAL(3,2) NOT NULL,
    sentiment_label VARCHAR(20) NOT NULL,
    confidence_score DECIMAL(3,2) NOT NULL,
    positive_keywords JSONB,
    negative_keywords JSONB,
    neutral_keywords JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE news_impact (
    id SERIAL PRIMARY KEY,
    news_id INTEGER REFERENCES news_articles(id),
    stock_code VARCHAR(10) NOT NULL,
    relevance_score DECIMAL(3,2) NOT NULL,
    sentiment_impact DECIMAL(3,2) NOT NULL,
    market_context DECIMAL(3,2) NOT NULL,
    total_impact DECIMAL(3,2) NOT NULL,
    expected_price_change DECIMAL(5,2),
    confidence_score DECIMAL(3,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE news_keywords (
    id SERIAL PRIMARY KEY,
    news_id INTEGER REFERENCES news_articles(id),
    keyword VARCHAR(100) NOT NULL,
    frequency INTEGER DEFAULT 1,
    importance_score DECIMAL(3,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE news_categories (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) UNIQUE NOT NULL,
    description TEXT,
    priority INTEGER DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ”„ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì²˜ë¦¬

### ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ
```python
# web/services/news_streaming_service.py
class NewsStreamingService:
    def __init__(self):
        self.websocket_manager = WebSocketManager()
        self.news_queue = Queue()
        self.processing_thread = None
    
    def start_streaming(self):
        """ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        self.processing_thread = Thread(target=self.process_news_stream)
        self.processing_thread.daemon = True
        self.processing_thread.start()
    
    def process_news_stream(self):
        """ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        while True:
            try:
                # ìƒˆë¡œìš´ ë‰´ìŠ¤ í™•ì¸
                new_news = self.check_for_new_news()
                
                for news in new_news:
                    # ì‹¤ì‹œê°„ ì²˜ë¦¬
                    processed_news = self.process_news_realtime(news)
                    
                    # ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                    self.broadcast_news(processed_news)
                    
                    # íì— ì¶”ê°€
                    self.news_queue.put(processed_news)
                
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸
                
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def process_news_realtime(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            processed_news = {
                'id': str(uuid.uuid4()),
                'title': news_data['title'],
                'content': news_data['content'][:200] + '...' if len(news_data['content']) > 200 else news_data['content'],
                'url': news_data['url'],
                'source': news_data['source'],
                'published_at': news_data['published_at'],
                'category': news_data['category'],
                'is_breaking': self.is_breaking_news(news_data),
                'processed_at': datetime.now().isoformat()
            }
            
            # ê°ì • ë¶„ì„
            sentiment_service = SentimentAnalysisService()
            sentiment_result = sentiment_service.analyze_news_sentiment(news_data)
            processed_news['sentiment'] = sentiment_result
            
            # ì˜í–¥ë„ ë¶„ì„
            impact_service = NewsImpactService()
            impact_result = impact_service.analyze_news_impact(news_data)
            processed_news['impact'] = impact_result
            
            return processed_news
            
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return news_data
    
    def broadcast_news(self, news_data: Dict[str, Any]):
        """ë‰´ìŠ¤ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        try:
            message = {
                'type': 'news_update',
                'data': news_data,
                'timestamp': datetime.now().isoformat()
            }
            
            self.websocket_manager.broadcast(message)
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def is_breaking_news(self, news_data: Dict[str, Any]) -> bool:
        """ê¸´ê¸‰ ë‰´ìŠ¤ íŒë³„"""
        breaking_keywords = ['ê¸´ê¸‰', 'ì†ë³´', 'BREAKING', 'URGENT', 'ê¸´ê¸‰ì†ë³´']
        title = news_data['title'].lower()
        
        return any(keyword.lower() in title for keyword in breaking_keywords)
```

## ğŸ” ë³´ì•ˆ ë° ê²€ì¦

### ë‰´ìŠ¤ ë°ì´í„° ê²€ì¦
```python
# web/services/news_validation_service.py
class NewsValidationService:
    def __init__(self):
        self.spam_filters = self.load_spam_filters()
        self.content_filters = self.load_content_filters()
    
    def validate_news_article(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ì¦"""
        try:
            validation_result = {
                'is_valid': True,
                'warnings': [],
                'errors': []
            }
            
            # ê¸°ë³¸ í•„ë“œ ê²€ì¦
            if not self.validate_required_fields(news_data):
                validation_result['is_valid'] = False
                validation_result['errors'].append('í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤')
            
            # ìŠ¤íŒ¸ í•„í„°ë§
            if self.is_spam_news(news_data):
                validation_result['is_valid'] = False
                validation_result['errors'].append('ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜ëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤')
            
            # ë‚´ìš© í’ˆì§ˆ ê²€ì¦
            quality_score = self.assess_content_quality(news_data)
            if quality_score < 0.5:
                validation_result['warnings'].append('ë‰´ìŠ¤ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤')
            
            # ì¤‘ë³µ ê²€ì‚¬
            if self.is_duplicate_news(news_data):
                validation_result['warnings'].append('ì¤‘ë³µëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤')
            
            return validation_result
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'is_valid': False,
                'errors': [f'ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}']
            }
    
    def validate_required_fields(self, news_data: Dict[str, Any]) -> bool:
        """í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
        required_fields = ['title', 'content', 'url', 'source']
        
        for field in required_fields:
            if field not in news_data or not news_data[field]:
                return False
        
        return True
    
    def is_spam_news(self, news_data: Dict[str, Any]) -> bool:
        """ìŠ¤íŒ¸ ë‰´ìŠ¤ íŒë³„"""
        text = f"{news_data['title']} {news_data['content']}"
        
        # ìŠ¤íŒ¸ í‚¤ì›Œë“œ ê²€ì‚¬
        spam_keywords = ['ê´‘ê³ ', 'í™ë³´', 'ì´ë²¤íŠ¸', 'í• ì¸', 'ë¬´ë£Œ']
        spam_count = sum(1 for keyword in spam_keywords if keyword in text)
        
        # ìŠ¤íŒ¸ ë¹„ìœ¨ ê³„ì‚°
        spam_ratio = spam_count / len(text.split())
        
        return spam_ratio > 0.1  # 10% ì´ìƒì´ë©´ ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜
    
    def assess_content_quality(self, news_data: Dict[str, Any]) -> float:
        """ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€"""
        text = f"{news_data['title']} {news_data['content']}"
        
        # ê¸¸ì´ ì ìˆ˜
        length_score = min(len(text) / 1000, 1.0)
        
        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì ìˆ˜
        words = text.split()
        unique_words = set(words)
        diversity_score = len(unique_words) / len(words) if words else 0
        
        # ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜
        sentences = text.split('.')
        structure_score = len([s for s in sentences if len(s.split()) > 5]) / len(sentences) if sentences else 0
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality_score = (length_score * 0.4 + diversity_score * 0.3 + structure_score * 0.3)
        
        return round(quality_score, 3)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ë‰´ìŠ¤ ìºì‹± ì‹œìŠ¤í…œ
```python
# web/services/news_cache_service.py
class NewsCacheService:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def cache_news_data(self, news_data: List[Dict[str, Any]]):
        """ë‰´ìŠ¤ ë°ì´í„° ìºì‹±"""
        try:
            cache_key = "latest_news"
            self.redis_client.setex(cache_key, 1800, json.dumps(news_data))  # 30ë¶„
            
            # ê°œë³„ ë‰´ìŠ¤ ìºì‹±
            for news in news_data:
                news_key = f"news:{news.get('id', '')}"
                self.redis_client.setex(news_key, 3600, json.dumps(news))  # 1ì‹œê°„
                
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìºì‹± ì‹¤íŒ¨: {e}")
    
    def get_cached_news(self, limit: int = 50) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ë‰´ìŠ¤ ì¡°íšŒ"""
        try:
            cache_key = "latest_news"
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                news_list = json.loads(cached_data)
                return news_list[:limit]
            
            return []
            
        except Exception as e:
            logger.error(f"ìºì‹œëœ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def cache_sentiment_analysis(self, news_id: str, sentiment_data: Dict[str, Any]):
        """ê°ì • ë¶„ì„ ê²°ê³¼ ìºì‹±"""
        try:
            cache_key = f"sentiment:{news_id}"
            self.redis_client.setex(cache_key, 7200, json.dumps(sentiment_data))  # 2ì‹œê°„
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ìºì‹± ì‹¤íŒ¨: {e}")
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
# tests/test_news_collection_service.py
import pytest
from web.services.news_collection_service import NewsCollectionService

class TestNewsCollectionService:
    def test_collect_real_time_news(self):
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        service = NewsCollectionService()
        
        news_list = service.collect_real_time_news()
        
        assert isinstance(news_list, list)
        if news_list:
            assert 'title' in news_list[0]
            assert 'content' in news_list[0]
            assert 'url' in news_list[0]
            assert 'source' in news_list[0]
    
    def test_process_news(self):
        """ë‰´ìŠ¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        service = NewsCollectionService()
        
        raw_news = [{
            'title': 'ì‚¼ì„±ì „ì ì‹¤ì  ë°œí‘œ',
            'content': 'ì‚¼ì„±ì „ìê°€ ì¢‹ì€ ì‹¤ì ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.',
            'url': 'https://example.com/news1',
            'published_at': '2024-01-01T10:00:00Z'
        }]
        
        processed_news = service.process_news(raw_news, 'test_source')
        
        assert len(processed_news) > 0
        assert processed_news[0]['category'] == 'earnings'
        assert '005930' in processed_news[0]['related_stocks']
```

## ğŸš€ ë°°í¬ ë° ìš´ì˜

### í™˜ê²½ ì„¤ì •
```python
# web/config/news_config.py
class NewsConfig:
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¤ì •
    NEWS_UPDATE_INTERVAL = 300  # 5ë¶„
    MAX_NEWS_PER_UPDATE = 100
    NEWS_RETENTION_DAYS = 30
    
    # ê°ì • ë¶„ì„ ì„¤ì •
    SENTIMENT_CONFIDENCE_THRESHOLD = 0.7
    SENTIMENT_UPDATE_INTERVAL = 600  # 10ë¶„
    
    # ìºì‹œ ì„¤ì •
    NEWS_CACHE_TTL = 1800  # 30ë¶„
    SENTIMENT_CACHE_TTL = 7200  # 2ì‹œê°„
    
    # API ì„¤ì •
    NEWS_API_KEYS = {
        'naver': os.environ.get('NAVER_NEWS_API_KEY'),
        'yahoo': os.environ.get('YAHOO_FINANCE_API_KEY'),
        'reuters': os.environ.get('REUTERS_API_KEY')
    }
    
    # í•„í„°ë§ ì„¤ì •
    SPAM_THRESHOLD = 0.1
    QUALITY_THRESHOLD = 0.5
    DUPLICATE_SIMILARITY_THRESHOLD = 0.8
```

## ğŸ“Š ë¶„ì„ ë° ê°œì„ 

### ë‰´ìŠ¤ ì„±ëŠ¥ ë¶„ì„
```python
# web/services/news_analytics_service.py
class NewsAnalyticsService:
    def analyze_news_performance(self) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        return {
            'collection_stats': self.get_collection_stats(),
            'sentiment_trends': self.analyze_sentiment_trends(),
            'impact_analysis': self.analyze_impact_performance(),
            'user_engagement': self.calculate_user_engagement()
        }
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìˆ˜ì§‘ í†µê³„"""
        query = """
            SELECT 
                source,
                COUNT(*) as article_count,
                AVG(sentiment_score) as avg_sentiment,
                COUNT(CASE WHEN sentiment_label = 'positive' THEN 1 END) as positive_count,
                COUNT(CASE WHEN sentiment_label = 'negative' THEN 1 END) as negative_count
            FROM news_articles na
            JOIN news_sentiment ns ON na.id = ns.news_id
            WHERE na.created_at >= CURRENT_DATE - INTERVAL '7 days'
            GROUP BY source
        """
        
        results = self.db.execute(query)
        
        stats = {}
        for row in results:
            stats[row['source']] = {
                'article_count': row['article_count'],
                'avg_sentiment': row['avg_sentiment'],
                'positive_ratio': row['positive_count'] / row['article_count'] if row['article_count'] > 0 else 0,
                'negative_ratio': row['negative_count'] / row['article_count'] if row['article_count'] > 0 else 0
            }
        
        return stats
```

## ğŸ”„ í–¥í›„ ê°œì„  ê³„íš

### ë‹¨ê¸° ê°œì„ ì‚¬í•­ (1-2ê°œì›”)
1. **ì‹¤ì‹œê°„ ë²ˆì—­** ê¸°ëŠ¥ ì¶”ê°€
2. **ë‰´ìŠ¤ ìš”ì•½** AI ìƒì„±
3. **ê°œì¸í™”ëœ ë‰´ìŠ¤** ì¶”ì²œ
4. **ë‰´ìŠ¤ ì•Œë¦¼** ì‹œìŠ¤í…œ

### ì¤‘ê¸° ê°œì„ ì‚¬í•­ (3-6ê°œì›”)
1. **ë©€í‹°ë¯¸ë””ì–´ ë‰´ìŠ¤** ì§€ì›
2. **ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹ ë¢°ë„** í‰ê°€
3. **ë‰´ìŠ¤ íŠ¸ë Œë“œ** ë¶„ì„
4. **ë‰´ìŠ¤ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤** ìµœì í™”

### ì¥ê¸° ê°œì„ ì‚¬í•­ (6ê°œì›” ì´ìƒ)
1. **AI ë‰´ìŠ¤ ìƒì„±** ì‹œìŠ¤í…œ
2. **ë‰´ìŠ¤ ì˜ˆì¸¡** ëª¨ë¸
3. **ê¸€ë¡œë²Œ ë‰´ìŠ¤** í†µí•©
4. **ë‰´ìŠ¤ ê¸°ë°˜ ìë™ ê±°ë˜** ì‹œìŠ¤í…œ

## ğŸ“š ì°¸ê³  ìë£Œ

### ê¸°ìˆ  ìŠ¤íƒ
- **í”„ë¡ íŠ¸ì—”ë“œ**: HTML5, CSS3, JavaScript (ES6+)
- **ë°±ì—”ë“œ**: Python Flask
- **AI/ML**: OpenAI GPT, TensorFlow, scikit-learn
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL, Redis
- **ì‹¤ì‹œê°„ í†µì‹ **: WebSocket, Socket.IO
- **í…ìŠ¤íŠ¸ ë¶„ì„**: KoNLPy, NLTK

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë„êµ¬
- **ë‰´ìŠ¤ ìˆ˜ì§‘**: BeautifulSoup, Scrapy
- **ê°ì • ë¶„ì„**: VADER, TextBlob
- **í…ìŠ¤íŠ¸ ì²˜ë¦¬**: spaCy, Transformers
- **ì‹œê°í™”**: Chart.js, D3.js
- **í…ŒìŠ¤íŠ¸**: pytest, Selenium
- **ëª¨ë‹ˆí„°ë§**: Prometheus, Grafana

ì´ ë¬¸ì„œëŠ” Pixie íˆ¬ìì±—ë´‡ì˜ ë‰´ìŠ¤/ì´ìŠˆ ì‹œìŠ¤í…œ ê°œë°œ ê³¼ì •ê³¼ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ ìƒì„¸íˆ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ë‰´ìŠ¤ í’ˆì§ˆ í–¥ìƒê³¼ ë¶„ì„ ì •í™•ë„ ê°œì„ ì„ í†µí•´ íˆ¬ììì—ê²Œ ë”ìš± ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. 